{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tech Challenge Fase 4 - Video Analysis (Google Colab Version)\n",
    "Este notebook foi gerado automaticamente para execução no Google Colab.\n",
    "Ele contém todo o código fonte necessário dos módulos e a lógica de análise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting ultralytics\n",
      "  Downloading ultralytics-8.3.249-py3-none-any.whl.metadata (37 kB)\n",
      "Collecting fer\n",
      "  Using cached fer-25.10.3-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting opencv-python-headless\n",
      "  Downloading opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.9.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pandas in /home/aineto/.local/lib/python3.9/site-packages (2.3.3)\n",
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.23.0 in /home/aineto/.local/lib/python3.9/site-packages (from ultralytics) (2.0.2)\n",
      "Collecting pillow>=7.1.2 (from ultralytics)\n",
      "  Using cached pillow-11.3.0-cp39-cp39-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /home/aineto/.local/lib/python3.9/site-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in /home/aineto/.local/lib/python3.9/site-packages (from ultralytics) (2.32.5)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /home/aineto/.local/lib/python3.9/site-packages (from ultralytics) (1.13.1)\n",
      "Requirement already satisfied: torch>=1.8.0 in /home/aineto/.local/lib/python3.9/site-packages (from ultralytics) (2.8.0)\n",
      "Collecting torchvision>=0.9.0 (from ultralytics)\n",
      "  Downloading torchvision-0.23.0-cp39-cp39-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /home/aineto/.local/lib/python3.9/site-packages (from ultralytics) (7.1.0)\n",
      "Collecting polars>=0.20.0 (from ultralytics)\n",
      "  Using cached polars-1.36.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting ultralytics-thop>=2.0.18 (from ultralytics)\n",
      "  Using cached ultralytics_thop-2.0.18-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting opencv-contrib-python (from fer)\n",
      "  Using cached opencv_contrib_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
      "Collecting tensorflow>=2.0.0 (from fer)\n",
      "  Using cached tensorflow-2.20.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
      "Collecting facenet-pytorch (from fer)\n",
      "  Using cached facenet_pytorch-2.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/aineto/.local/lib/python3.9/site-packages (from fer) (4.67.1)\n",
      "Collecting moviepy<2.0,>=1.0.3 (from fer)\n",
      "  Using cached moviepy-1.0.3-py3-none-any.whl\n",
      "Collecting ffmpeg-python>=0.2.0 (from fer)\n",
      "  Using cached ffmpeg_python-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting decorator<5.0,>=4.0.2 (from moviepy<2.0,>=1.0.3->fer)\n",
      "  Using cached decorator-4.4.2-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting imageio<3.0,>=2.5 (from moviepy<2.0,>=1.0.3->fer)\n",
      "  Using cached imageio-2.37.2-py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting imageio_ffmpeg>=0.2.0 (from moviepy<2.0,>=1.0.3->fer)\n",
      "  Using cached imageio_ffmpeg-0.6.0-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting proglog<=1.0.0 (from moviepy<2.0,>=1.0.3->fer)\n",
      "  Using cached proglog-0.1.12-py3-none-any.whl.metadata (794 bytes)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/aineto/.local/lib/python3.9/site-packages (from requests>=2.23.0->ultralytics) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/aineto/.local/lib/python3.9/site-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/aineto/.local/lib/python3.9/site-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/aineto/.local/lib/python3.9/site-packages (from requests>=2.23.0->ultralytics) (2025.8.3)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Using cached fonttools-4.60.2-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (113 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.7-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/aineto/.local/lib/python3.9/site-packages (from matplotlib) (25.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.3.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/aineto/.local/lib/python3.9/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting importlib-resources>=3.2.0 (from matplotlib)\n",
      "  Using cached importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/aineto/.local/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/aineto/.local/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Collecting future (from ffmpeg-python>=0.2.0->fer)\n",
      "  Using cached future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/aineto/.local/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.23.0)\n",
      "Collecting polars-runtime-32==1.36.1 (from polars>=0.20.0->ultralytics)\n",
      "  Using cached polars_runtime_32-1.36.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow>=2.0.0->fer)\n",
      "  Using cached absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow>=2.0.0->fer)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow>=2.0.0->fer)\n",
      "  Using cached flatbuffers-25.12.19-py2.py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow>=2.0.0->fer)\n",
      "  Using cached gast-0.7.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting google_pasta>=0.1.1 (from tensorflow>=2.0.0->fer)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow>=2.0.0->fer)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt_einsum>=2.3.2 (from tensorflow>=2.0.0->fer)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting protobuf>=5.28.0 (from tensorflow>=2.0.0->fer)\n",
      "  Using cached protobuf-6.33.2-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3.9/site-packages (from tensorflow>=2.0.0->fer) (53.0.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow>=2.0.0->fer)\n",
      "  Using cached termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in /home/aineto/.local/lib/python3.9/site-packages (from tensorflow>=2.0.0->fer) (4.15.0)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow>=2.0.0->fer)\n",
      "  Using cached wrapt-2.0.1-cp39-cp39-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow>=2.0.0->fer)\n",
      "  Using cached grpcio-1.76.0-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.7 kB)\n",
      "Collecting tensorboard~=2.20.0 (from tensorflow>=2.0.0->fer)\n",
      "  Using cached tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.10.0 (from tensorflow>=2.0.0->fer)\n",
      "  Using cached keras-3.10.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow>=2.0.0->fer)\n",
      "  Using cached h5py-3.14.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting ml_dtypes<1.0.0,>=0.5.1 (from tensorflow>=2.0.0->fer)\n",
      "  Using cached ml_dtypes-0.5.4-cp39-cp39-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.20.0->tensorflow>=2.0.0->fer)\n",
      "  Using cached markdown-3.9-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow>=2.0.0->fer)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.20.0->tensorflow>=2.0.0->fer)\n",
      "  Using cached werkzeug-3.1.4-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow>=2.0.0->fer)\n",
      "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: rich in /home/aineto/.local/lib/python3.9/site-packages (from keras>=3.10.0->tensorflow>=2.0.0->fer) (14.1.0)\n",
      "Collecting namex (from keras>=3.10.0->tensorflow>=2.0.0->fer)\n",
      "  Using cached namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.10.0->tensorflow>=2.0.0->fer)\n",
      "  Using cached optree-0.18.0-cp39-cp39-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (34 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/aineto/.local/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard~=2.20.0->tensorflow>=2.0.0->fer) (8.7.0)\n",
      "Requirement already satisfied: filelock in /home/aineto/.local/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (3.19.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/aineto/.local/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/aineto/.local/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/aineto/.local/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/aineto/.local/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/aineto/.local/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/aineto/.local/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/aineto/.local/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/aineto/.local/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/aineto/.local/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/aineto/.local/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/aineto/.local/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/aineto/.local/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/aineto/.local/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/aineto/.local/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/aineto/.local/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/aineto/.local/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/aineto/.local/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/aineto/.local/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/aineto/.local/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/aineto/.local/lib/python3.9/site-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /home/aineto/.local/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow>=2.0.0->fer) (3.0.3)\n",
      "INFO: pip is looking at multiple versions of facenet-pytorch to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting facenet-pytorch (from fer)\n",
      "  Using cached facenet_pytorch-2.5.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/aineto/.local/lib/python3.9/site-packages (from rich->keras>=3.10.0->tensorflow>=2.0.0->fer) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/aineto/.local/lib/python3.9/site-packages (from rich->keras>=3.10.0->tensorflow>=2.0.0->fer) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/aineto/.local/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow>=2.0.0->fer) (0.1.2)\n",
      "Downloading ultralytics-8.3.249-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached fer-25.10.3-py3-none-any.whl (891 kB)\n",
      "Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\n",
      "Using cached imageio-2.37.2-py3-none-any.whl (317 kB)\n",
      "Using cached proglog-0.1.12-py3-none-any.whl (6.3 kB)\n",
      "Downloading opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (54.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 MB\u001b[0m \u001b[31m105.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached matplotlib-3.9.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
      "Using cached opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (67.0 MB)\n",
      "Using cached contourpy-1.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (321 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
      "Using cached fonttools-4.60.2-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (4.8 MB)\n",
      "Using cached imageio_ffmpeg-0.6.0-py3-none-manylinux2014_x86_64.whl (29.5 MB)\n",
      "Using cached importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Using cached kiwisolver-1.4.7-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "Downloading pillow-11.3.0-cp39-cp39-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached polars-1.36.1-py3-none-any.whl (802 kB)\n",
      "Using cached polars_runtime_32-1.36.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (44.5 MB)\n",
      "Using cached pyparsing-3.3.1-py3-none-any.whl (121 kB)\n",
      "Using cached tensorflow-2.20.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.4 MB)\n",
      "Using cached grpcio-1.76.0-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (6.6 MB)\n",
      "Using cached ml_dtypes-0.5.4-cp39-cp39-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (5.0 MB)\n",
      "Using cached tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "Using cached absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Using cached flatbuffers-25.12.19-py2.py3-none-any.whl (26 kB)\n",
      "Using cached gast-0.7.0-py3-none-any.whl (22 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached h5py-3.14.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "Using cached keras-3.10.0-py3-none-any.whl (1.4 MB)\n",
      "Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "Using cached markdown-3.9-py3-none-any.whl (107 kB)\n",
      "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Using cached protobuf-6.33.2-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
      "Using cached termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading torchvision-0.23.0-cp39-cp39-manylinux_2_28_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached ultralytics_thop-2.0.18-py3-none-any.whl (28 kB)\n",
      "Using cached werkzeug-3.1.4-py3-none-any.whl (224 kB)\n",
      "Using cached wrapt-2.0.1-cp39-cp39-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (113 kB)\n",
      "Using cached facenet_pytorch-2.5.3-py3-none-any.whl (1.9 MB)\n",
      "Using cached future-1.0.0-py3-none-any.whl (491 kB)\n",
      "Using cached namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Using cached opencv_contrib_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (73.2 MB)\n",
      "Using cached optree-0.18.0-cp39-cp39-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (386 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, wheel, werkzeug, termcolor, tensorboard-data-server, pyparsing, protobuf, proglog, polars-runtime-32, pillow, optree, opt_einsum, opencv-python-headless, opencv-python, opencv-contrib-python, ml_dtypes, kiwisolver, importlib-resources, imageio_ffmpeg, h5py, grpcio, google_pasta, gast, future, fonttools, decorator, cycler, contourpy, absl-py, polars, matplotlib, markdown, imageio, ffmpeg-python, astunparse, tensorboard, moviepy, keras, ultralytics-thop, torchvision, tensorflow, ultralytics, facenet-pytorch, fer\n",
      "\u001b[2K  Attempting uninstall: decoratorm\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27/47\u001b[0m [fonttools]mpeg]python]]\n",
      "\u001b[2K    Found existing installation: decorator 5.2.1━━━━━━━━━━━━━━\u001b[0m \u001b[32m27/47\u001b[0m [fonttools]\n",
      "\u001b[2K    Uninstalling decorator-5.2.1:1m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27/47\u001b[0m [fonttools]\n",
      "\u001b[2K      Successfully uninstalled decorator-5.2.1[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28/47\u001b[0m [decorator]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47/47\u001b[0m [fer]32m45/47\u001b[0m [facenet-pytorch]]\n",
      "\u001b[1A\u001b[2KSuccessfully installed absl-py-2.3.1 astunparse-1.6.3 contourpy-1.3.0 cycler-0.12.1 decorator-4.4.2 facenet-pytorch-2.5.3 fer-25.10.3 ffmpeg-python-0.2.0 flatbuffers-25.12.19 fonttools-4.60.2 future-1.0.0 gast-0.7.0 google_pasta-0.2.0 grpcio-1.76.0 h5py-3.14.0 imageio-2.37.2 imageio_ffmpeg-0.6.0 importlib-resources-6.5.2 keras-3.10.0 kiwisolver-1.4.7 libclang-18.1.1 markdown-3.9 matplotlib-3.9.4 ml_dtypes-0.5.4 moviepy-1.0.3 namex-0.1.0 opencv-contrib-python-4.12.0.88 opencv-python-4.12.0.88 opencv-python-headless-4.12.0.88 opt_einsum-3.4.0 optree-0.18.0 pillow-11.3.0 polars-1.36.1 polars-runtime-32-1.36.1 proglog-0.1.12 protobuf-6.33.2 pyparsing-3.3.1 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 termcolor-3.1.0 torchvision-0.23.0 ultralytics-8.3.249 ultralytics-thop-2.0.18 werkzeug-3.1.4 wheel-0.45.1 wrapt-2.0.1\n",
      "zsh:1: command not found: apt-get\n"
     ]
    }
   ],
   "source": [
    "# Instalação das dependências\n",
    "!pip install ultralytics fer opencv-python-headless matplotlib pandas opencv-python\n",
    "# Instala ffmpeg para processamento de vídeo\n",
    "!apt-get install ffmpeg -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Montar Google Drive\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[32m      3\u001b[39m drive.mount(\u001b[33m'\u001b[39m\u001b[33m/content/drive\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mGoogle Drive montado com sucesso!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# Montar Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"Google Drive montado com sucesso!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Define diretório base no Colab\n",
    "BASE_DIR = Path(\"/content/TC-4\")\n",
    "SRC_DIR = BASE_DIR / \"src\"\n",
    "INPUT_DIR = BASE_DIR / \"input\"\n",
    "OUTPUT_DIR = BASE_DIR / \"output\"\n",
    "REPORTS_DIR = BASE_DIR / \"reports\"\n",
    "MODELS_DIR = BASE_DIR / \"models\"\n",
    "\n",
    "# Cria estrutura de pastas\n",
    "for d in [SRC_DIR, INPUT_DIR, OUTPUT_DIR, REPORTS_DIR, MODELS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Adiciona ao path para permitir importações\n",
    "if str(BASE_DIR) not in sys.path:\n",
    "    sys.path.append(str(BASE_DIR))\n",
    "\n",
    "print(f\"Ambiente configurado em: {BASE_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload do Vídeo\n",
    "Você pode fazer upload do vídeo manualmente para a pasta `/content/TC-4/input` ou copiar do Google Drive.\n",
    "Abaixo, um exemplo para copiar do Drive (ajuste o caminho de origem conforme necessário)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de cópia do Drive (descomente e ajuste se necessário)\n",
    "# !cp \"/content/drive/MyDrive/TechChallenge4/video_teste.mp4\" \"/content/TC-4/input/video_input.mp4\"\n",
    "\n",
    "# Verificação do arquivo\n",
    "video_files = list(INPUT_DIR.glob(\"*.mp4\"))\n",
    "if video_files:\n",
    "    VIDEO_PATH_FOUND = str(video_files[0])\n",
    "    print(f\"Vídeo encontrado: {VIDEO_PATH_FOUND}\")\n",
    "else:\n",
    "    print(\"Nenhum vídeo .mp4 encontrado em input/. Faça upload do arquivo.\")\n",
    "    VIDEO_PATH_FOUND = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /content/TC-4/src/config.py\n",
    "\"\"\"\n",
    "Tech Challenge - Fase 4: Configurações do Projeto\n",
    "Centraliza todas as configurações e constantes utilizadas na aplicação.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Diretórios\n",
    "BASE_DIR = Path(\"/content/TC-4\")\n",
    "SRC_DIR = BASE_DIR / \"src\"\n",
    "INPUT_DIR = BASE_DIR / \"input\"\n",
    "OUTPUT_DIR = BASE_DIR / \"output\"\n",
    "REPORTS_DIR = BASE_DIR / \"reports\"\n",
    "MODELS_DIR = BASE_DIR / \"models\"\n",
    "\n",
    "# Criar diretórios se não existirem\n",
    "INPUT_DIR.mkdir(exist_ok=True)\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "REPORTS_DIR.mkdir(exist_ok=True)\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Vídeo de entrada\n",
    "VIDEO_PATH = os.getenv(\n",
    "    \"VIDEO_PATH\", \n",
    "    str(INPUT_DIR / \"Unlocking Facial Recognition_ Diverse Activities Analysis.mp4\")\n",
    ")\n",
    "\n",
    "# Configurações de processamento\n",
    "FRAME_SKIP = int(os.getenv(\"FRAME_SKIP\", \"2\"))\n",
    "CONFIDENCE_THRESHOLD = float(os.getenv(\"CONFIDENCE_THRESHOLD\", \"0.5\"))\n",
    "\n",
    "# OpenAI (opcional para geração de resumo)\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
    "\n",
    "# Configurações de visualização\n",
    "COLORS = {\n",
    "    \"face\": (0, 255, 0),       # Verde para rostos\n",
    "    \"emotion\": (255, 255, 0),  # Amarelo para emoções\n",
    "    \"activity\": (0, 165, 255), # Laranja para atividades\n",
    "    \"anomaly\": (0, 0, 255),    # Vermelho para anomalias\n",
    "    \"text_bg\": (0, 0, 0),      # Fundo preto para texto\n",
    "}\n",
    "\n",
    "# Mapeamento de emoções (português)\n",
    "EMOTION_LABELS = {\n",
    "    \"angry\": \"Raiva\",\n",
    "    \"disgust\": \"Nojo\",\n",
    "    \"fear\": \"Medo\",\n",
    "    \"happy\": \"Feliz\",\n",
    "    \"sad\": \"Triste\",\n",
    "    \"surprise\": \"Surpreso\",\n",
    "    \"neutral\": \"Neutro\"\n",
    "}\n",
    "\n",
    "# Categorias de atividades detectáveis\n",
    "ACTIVITY_CATEGORIES = {\n",
    "    \"walking\": \"Caminhando\",\n",
    "    \"running\": \"Correndo\",\n",
    "    \"sitting\": \"Sentado\",\n",
    "    \"standing\": \"Em pé\",\n",
    "    \"talking\": \"Conversando\",\n",
    "    \"gesturing\": \"Gesticulando\",\n",
    "    \"waving\": \"Acenando\",\n",
    "    \"pointing\": \"Apontando\",\n",
    "    \"dancing\": \"Dançando\",\n",
    "    \"crouching\": \"Agachado\",\n",
    "    \"arms_raised\": \"Braços Levantados\",\n",
    "    \"unknown\": \"Desconhecido\"\n",
    "}\n",
    "\n",
    "# Limiares para detecção de anomalias\n",
    "ANOMALY_THRESHOLDS = {\n",
    "    \"sudden_movement\": 50,      # Pixels de movimento brusco\n",
    "    \"emotion_change_rate\": 0.5, # Taxa de mudança emocional\n",
    "    \"activity_duration\": 2.0,   # Segundos mínimos para atividade válida\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /content/TC-4/src/face_detector.py\n",
    "\"\"\"\n",
    "Tech Challenge - Fase 4: Detector de Rostos\n",
    "Módulo responsável pelo reconhecimento e rastreamento de rostos no vídeo.\n",
    "\"\"\"\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FaceDetection:\n",
    "    \"\"\"Representa uma detecção de rosto em um frame.\"\"\"\n",
    "    face_id: int\n",
    "    bbox: Tuple[int, int, int, int]  # (x, y, w, h)\n",
    "    confidence: float\n",
    "    landmarks: Optional[Dict] = None\n",
    "    embedding: Optional[np.ndarray] = None\n",
    "\n",
    "\n",
    "class FaceDetector:\n",
    "    \"\"\"\n",
    "    Detector de rostos usando OpenCV DNN ou Haar Cascades.\n",
    "    Suporta rastreamento básico de identidades entre frames.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, method: str = \"haar\", confidence_threshold: float = 0.5):\n",
    "        \"\"\"\n",
    "        Inicializa o detector de rostos.\n",
    "        \n",
    "        Args:\n",
    "            method: Método de detecção ('haar', 'dnn', 'mediapipe')\n",
    "            confidence_threshold: Limiar mínimo de confiança\n",
    "        \"\"\"\n",
    "        self.method = method\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.face_counter = 0\n",
    "        self.tracked_faces: Dict[int, np.ndarray] = {}\n",
    "        \n",
    "        self._init_detector()\n",
    "    \n",
    "    def _init_detector(self):\n",
    "        \"\"\"Inicializa o detector baseado no método escolhido.\"\"\"\n",
    "        if self.method == \"haar\":\n",
    "            # Haar Cascade - usa caminho explícito para evitar problemas de cache\n",
    "            import os\n",
    "            from pathlib import Path\n",
    "            \n",
    "            # Tenta múltiplos caminhos possíveis\n",
    "            possible_paths = [\n",
    "                # Caminho do cv2 atual\n",
    "                str(Path(cv2.__file__).parent / \"data\" / \"haarcascade_frontalface_default.xml\"),\n",
    "                # Caminho do venv Python 3.12\n",
    "                \"/home/aineto/workspaces/POS/TC-4/.venv/lib/python3.12/site-packages/cv2/data/haarcascade_frontalface_default.xml\",\n",
    "                # Caminho do FER\n",
    "                \"/home/aineto/workspaces/POS/TC-4/.venv/lib/python3.12/site-packages/fer/data/haarcascade_frontalface_default.xml\",\n",
    "                # Caminhos do sistema\n",
    "                \"/usr/share/opencv4/haarcascades/haarcascade_frontalface_default.xml\",\n",
    "                \"/usr/share/opencv/haarcascades/haarcascade_frontalface_default.xml\",\n",
    "            ]\n",
    "            \n",
    "            for path in possible_paths:\n",
    "                if os.path.exists(path):\n",
    "                    self.detector = cv2.CascadeClassifier(path)\n",
    "                    if not self.detector.empty():\n",
    "                        break\n",
    "            \n",
    "            if self.detector.empty():\n",
    "                print(\"[AVISO] Haar Cascade não encontrado, usando método simplificado\")\n",
    "        elif self.method == \"dnn\":\n",
    "            # DNN (mais preciso, requer modelo)\n",
    "            self._init_dnn_detector()\n",
    "        elif self.method == \"mediapipe\":\n",
    "            self._init_mediapipe_detector()\n",
    "        else:\n",
    "            raise ValueError(f\"Método desconhecido: {self.method}\")\n",
    "    \n",
    "    def _init_dnn_detector(self):\n",
    "        \"\"\"Inicializa detector DNN do OpenCV.\"\"\"\n",
    "        # Usa modelo Caffe pré-treinado do OpenCV\n",
    "        model_path = cv2.data.haarcascades.replace(\n",
    "            \"haarcascades/\", \"\"\n",
    "        ) + \"deploy.prototxt\"\n",
    "        weights_path = cv2.data.haarcascades.replace(\n",
    "            \"haarcascades/\", \"\"\n",
    "        ) + \"res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "        \n",
    "        try:\n",
    "            self.detector = cv2.dnn.readNetFromCaffe(model_path, weights_path)\n",
    "        except Exception:\n",
    "            print(\"[AVISO] Modelo DNN não encontrado, usando Haar Cascade\")\n",
    "            self.method = \"haar\"\n",
    "            self._init_detector()\n",
    "    \n",
    "    def _init_mediapipe_detector(self):\n",
    "        \"\"\"Inicializa detector MediaPipe Face Detection.\"\"\"\n",
    "        try:\n",
    "            import mediapipe as mp\n",
    "            self.mp_face_detection = mp.solutions.face_detection\n",
    "            self.detector = self.mp_face_detection.FaceDetection(\n",
    "                model_selection=1,  # 1 para detecção de longa distância\n",
    "                min_detection_confidence=self.confidence_threshold\n",
    "            )\n",
    "        except ImportError:\n",
    "            print(\"[AVISO] MediaPipe não instalado, usando Haar Cascade\")\n",
    "            self.method = \"haar\"\n",
    "            self._init_detector()\n",
    "    \n",
    "    def detect(self, frame: np.ndarray) -> List[FaceDetection]:\n",
    "        \"\"\"\n",
    "        Detecta rostos em um frame.\n",
    "        \n",
    "        Args:\n",
    "            frame: Imagem BGR do OpenCV\n",
    "            \n",
    "        Returns:\n",
    "            Lista de detecções de rostos\n",
    "        \"\"\"\n",
    "        if self.method == \"haar\":\n",
    "            return self._detect_haar(frame)\n",
    "        elif self.method == \"dnn\":\n",
    "            return self._detect_dnn(frame)\n",
    "        elif self.method == \"mediapipe\":\n",
    "            return self._detect_mediapipe(frame)\n",
    "        return []\n",
    "    \n",
    "    def _detect_haar(self, frame: np.ndarray) -> List[FaceDetection]:\n",
    "        \"\"\"Detecção usando Haar Cascades.\"\"\"\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = self.detector.detectMultiScale(\n",
    "            gray,\n",
    "            scaleFactor=1.1,\n",
    "            minNeighbors=5,\n",
    "            minSize=(30, 30)\n",
    "        )\n",
    "        \n",
    "        detections = []\n",
    "        for (x, y, w, h) in faces:\n",
    "            face_id = self._assign_face_id(frame, (x, y, w, h))\n",
    "            detections.append(FaceDetection(\n",
    "                face_id=face_id,\n",
    "                bbox=(x, y, w, h),\n",
    "                confidence=1.0  # Haar não fornece confiança\n",
    "            ))\n",
    "        \n",
    "        return detections\n",
    "    \n",
    "    def _detect_dnn(self, frame: np.ndarray) -> List[FaceDetection]:\n",
    "        \"\"\"Detecção usando DNN do OpenCV.\"\"\"\n",
    "        h, w = frame.shape[:2]\n",
    "        blob = cv2.dnn.blobFromImage(\n",
    "            cv2.resize(frame, (300, 300)), \n",
    "            1.0, (300, 300), (104.0, 177.0, 123.0)\n",
    "        )\n",
    "        \n",
    "        self.detector.setInput(blob)\n",
    "        detections_raw = self.detector.forward()\n",
    "        \n",
    "        detections = []\n",
    "        for i in range(detections_raw.shape[2]):\n",
    "            confidence = detections_raw[0, 0, i, 2]\n",
    "            if confidence > self.confidence_threshold:\n",
    "                box = detections_raw[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "                x1, y1, x2, y2 = box.astype(int)\n",
    "                face_id = self._assign_face_id(frame, (x1, y1, x2-x1, y2-y1))\n",
    "                detections.append(FaceDetection(\n",
    "                    face_id=face_id,\n",
    "                    bbox=(x1, y1, x2-x1, y2-y1),\n",
    "                    confidence=float(confidence)\n",
    "                ))\n",
    "        \n",
    "        return detections\n",
    "    \n",
    "    def _detect_mediapipe(self, frame: np.ndarray) -> List[FaceDetection]:\n",
    "        \"\"\"Detecção usando MediaPipe.\"\"\"\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = self.detector.process(rgb_frame)\n",
    "        \n",
    "        detections = []\n",
    "        if results.detections:\n",
    "            h, w = frame.shape[:2]\n",
    "            for detection in results.detections:\n",
    "                bbox = detection.location_data.relative_bounding_box\n",
    "                x = int(bbox.xmin * w)\n",
    "                y = int(bbox.ymin * h)\n",
    "                width = int(bbox.width * w)\n",
    "                height = int(bbox.height * h)\n",
    "                \n",
    "                face_id = self._assign_face_id(frame, (x, y, width, height))\n",
    "                detections.append(FaceDetection(\n",
    "                    face_id=face_id,\n",
    "                    bbox=(x, y, width, height),\n",
    "                    confidence=detection.score[0]\n",
    "                ))\n",
    "        \n",
    "        return detections\n",
    "    \n",
    "    def _assign_face_id(self, frame: np.ndarray, bbox: Tuple[int, int, int, int]) -> int:\n",
    "        \"\"\"\n",
    "        Atribui um ID ao rosto baseado em rastreamento simples por posição.\n",
    "        Para rastreamento mais robusto, usar embeddings faciais.\n",
    "        \"\"\"\n",
    "        x, y, w, h = bbox\n",
    "        center = np.array([x + w/2, y + h/2])\n",
    "        \n",
    "        # Procura rosto mais próximo já rastreado\n",
    "        min_dist = float('inf')\n",
    "        matched_id = None\n",
    "        \n",
    "        for face_id, prev_center in self.tracked_faces.items():\n",
    "            dist = np.linalg.norm(center - prev_center)\n",
    "            if dist < min_dist and dist < max(w, h) * 2:  # Threshold de proximidade\n",
    "                min_dist = dist\n",
    "                matched_id = face_id\n",
    "        \n",
    "        if matched_id is not None:\n",
    "            self.tracked_faces[matched_id] = center\n",
    "            return matched_id\n",
    "        \n",
    "        # Novo rosto detectado\n",
    "        self.face_counter += 1\n",
    "        self.tracked_faces[self.face_counter] = center\n",
    "        return self.face_counter\n",
    "    \n",
    "    def reset_tracking(self):\n",
    "        \"\"\"Reseta o rastreamento de rostos.\"\"\"\n",
    "        self.face_counter = 0\n",
    "        self.tracked_faces.clear()\n",
    "    \n",
    "    def draw_detections(\n",
    "        self, \n",
    "        frame: np.ndarray, \n",
    "        detections: List[FaceDetection],\n",
    "        color: Tuple[int, int, int] = (0, 255, 0)\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Desenha as detecções de rostos no frame.\n",
    "        \n",
    "        Args:\n",
    "            frame: Imagem BGR\n",
    "            detections: Lista de detecções\n",
    "            color: Cor das bounding boxes (BGR)\n",
    "            \n",
    "        Returns:\n",
    "            Frame com anotações\n",
    "        \"\"\"\n",
    "        annotated = frame.copy()\n",
    "        \n",
    "        for det in detections:\n",
    "            x, y, w, h = det.bbox\n",
    "            \n",
    "            # Bounding box\n",
    "            cv2.rectangle(annotated, (x, y), (x+w, y+h), color, 2)\n",
    "            \n",
    "            # Label com ID e confiança\n",
    "            label = f\"Face #{det.face_id}\"\n",
    "            if det.confidence < 1.0:\n",
    "                label += f\" ({det.confidence:.0%})\"\n",
    "            \n",
    "            # Fundo do texto\n",
    "            (text_w, text_h), _ = cv2.getTextSize(\n",
    "                label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1\n",
    "            )\n",
    "            cv2.rectangle(\n",
    "                annotated, \n",
    "                (x, y - text_h - 10), \n",
    "                (x + text_w + 4, y), \n",
    "                color, \n",
    "                -1\n",
    "            )\n",
    "            \n",
    "            # Texto\n",
    "            cv2.putText(\n",
    "                annotated, label,\n",
    "                (x + 2, y - 5),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.6,\n",
    "                (0, 0, 0), 1, cv2.LINE_AA\n",
    "            )\n",
    "        \n",
    "        return annotated\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /content/TC-4/src/emotion_analyzer.py\n",
    "\"\"\"\n",
    "Tech Challenge - Fase 4: Analisador de Emoções\n",
    "Módulo responsável pela análise de expressões emocionais em rostos detectados.\n",
    "\"\"\"\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from collections import deque\n",
    "\n",
    "from .config import EMOTION_LABELS\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EmotionResult:\n",
    "    \"\"\"Resultado da análise emocional de um rosto.\"\"\"\n",
    "    face_id: int\n",
    "    dominant_emotion: str\n",
    "    emotion_scores: Dict[str, float]\n",
    "    confidence: float\n",
    "    emotion_pt: str  # Emoção em português\n",
    "\n",
    "\n",
    "class EmotionAnalyzer:\n",
    "    \"\"\"\n",
    "    Analisador de expressões emocionais usando DeepFace ou FER.\n",
    "    Suporta rastreamento temporal de emoções para suavização.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, method: str = \"fer\", temporal_window: int = 5):\n",
    "        \"\"\"\n",
    "        Inicializa o analisador de emoções.\n",
    "        \n",
    "        Args:\n",
    "            method: Método de análise ('fer', 'deepface')\n",
    "            temporal_window: Janela temporal para suavização (em frames)\n",
    "        \"\"\"\n",
    "        self.method = method\n",
    "        self.temporal_window = temporal_window\n",
    "        self.emotion_history: Dict[int, deque] = {}  # face_id -> histórico\n",
    "        \n",
    "        self._init_analyzer()\n",
    "    \n",
    "    def _init_analyzer(self):\n",
    "        \"\"\"Inicializa o analisador baseado no método escolhido.\"\"\"\n",
    "        if self.method == \"fer\":\n",
    "            self._init_fer()\n",
    "        elif self.method == \"deepface\":\n",
    "            self._init_deepface()\n",
    "        else:\n",
    "            raise ValueError(f\"Método desconhecido: {self.method}\")\n",
    "    \n",
    "    def _init_fer(self):\n",
    "        \"\"\"Inicializa FER (Facial Expression Recognition).\"\"\"\n",
    "        try:\n",
    "            # Nova versão do FER usa fer.fer.FER\n",
    "            try:\n",
    "                from fer.fer import FER\n",
    "            except ImportError:\n",
    "                from fer import FER\n",
    "            self.analyzer = FER(mtcnn=False)  # Usamos nosso próprio detector\n",
    "        except (ImportError, Exception) as e:\n",
    "            print(f\"[AVISO] FER não disponível ({e}), usando análise simplificada\")\n",
    "            self.method = \"simple\"\n",
    "            self.analyzer = None\n",
    "    \n",
    "    def _init_deepface(self):\n",
    "        \"\"\"Inicializa DeepFace para análise de emoções.\"\"\"\n",
    "        try:\n",
    "            from deepface import DeepFace\n",
    "            self.analyzer = DeepFace\n",
    "        except ImportError:\n",
    "            print(\"[AVISO] DeepFace não instalado, tentando FER\")\n",
    "            self.method = \"fer\"\n",
    "            self._init_fer()\n",
    "    \n",
    "    def analyze(\n",
    "        self, \n",
    "        frame: np.ndarray, \n",
    "        face_bbox: Tuple[int, int, int, int],\n",
    "        face_id: int\n",
    "    ) -> Optional[EmotionResult]:\n",
    "        \"\"\"\n",
    "        Analisa a emoção de um rosto no frame.\n",
    "        \n",
    "        Args:\n",
    "            frame: Imagem BGR\n",
    "            face_bbox: Bounding box do rosto (x, y, w, h)\n",
    "            face_id: ID do rosto para rastreamento temporal\n",
    "            \n",
    "        Returns:\n",
    "            Resultado da análise emocional ou None se falhar\n",
    "        \"\"\"\n",
    "        x, y, w, h = face_bbox\n",
    "        \n",
    "        # Extrai região do rosto com margem\n",
    "        margin = int(min(w, h) * 0.1)\n",
    "        x1 = max(0, x - margin)\n",
    "        y1 = max(0, y - margin)\n",
    "        x2 = min(frame.shape[1], x + w + margin)\n",
    "        y2 = min(frame.shape[0], y + h + margin)\n",
    "        \n",
    "        face_roi = frame[y1:y2, x1:x2]\n",
    "        \n",
    "        if face_roi.size == 0:\n",
    "            return None\n",
    "        \n",
    "        if self.method == \"fer\":\n",
    "            return self._analyze_fer(face_roi, face_id)\n",
    "        elif self.method == \"deepface\":\n",
    "            return self._analyze_deepface(face_roi, face_id)\n",
    "        else:\n",
    "            return self._analyze_simple(face_roi, face_id)\n",
    "    \n",
    "    def _analyze_fer(\n",
    "        self, \n",
    "        face_roi: np.ndarray, \n",
    "        face_id: int\n",
    "    ) -> Optional[EmotionResult]:\n",
    "        \"\"\"Análise usando FER.\"\"\"\n",
    "        if self.analyzer is None:\n",
    "            return self._analyze_simple(face_roi, face_id)\n",
    "        \n",
    "        try:\n",
    "            # FER espera imagem BGR\n",
    "            emotions = self.analyzer.detect_emotions(face_roi)\n",
    "            \n",
    "            if not emotions:\n",
    "                return None\n",
    "            \n",
    "            # Pega a primeira detecção (já sabemos que há um rosto)\n",
    "            emotion_scores = emotions[0][\"emotions\"]\n",
    "            \n",
    "            # Aplica suavização temporal\n",
    "            smoothed_scores = self._smooth_emotions(face_id, emotion_scores)\n",
    "            \n",
    "            # Encontra emoção dominante\n",
    "            dominant = max(smoothed_scores, key=smoothed_scores.get)\n",
    "            confidence = smoothed_scores[dominant]\n",
    "            \n",
    "            return EmotionResult(\n",
    "                face_id=face_id,\n",
    "                dominant_emotion=dominant,\n",
    "                emotion_scores=smoothed_scores,\n",
    "                confidence=confidence,\n",
    "                emotion_pt=EMOTION_LABELS.get(dominant, dominant)\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"[ERRO] FER: {e}\")\n",
    "            return self._analyze_simple(face_roi, face_id)\n",
    "    \n",
    "    def _analyze_deepface(\n",
    "        self, \n",
    "        face_roi: np.ndarray, \n",
    "        face_id: int\n",
    "    ) -> Optional[EmotionResult]:\n",
    "        \"\"\"Análise usando DeepFace.\"\"\"\n",
    "        try:\n",
    "            result = self.analyzer.analyze(\n",
    "                face_roi,\n",
    "                actions=[\"emotion\"],\n",
    "                enforce_detection=False,\n",
    "                silent=True\n",
    "            )\n",
    "            \n",
    "            if not result:\n",
    "                return None\n",
    "            \n",
    "            emotion_scores = result[0][\"emotion\"]\n",
    "            # Normaliza para 0-1\n",
    "            emotion_scores = {\n",
    "                k: v / 100.0 for k, v in emotion_scores.items()\n",
    "            }\n",
    "            \n",
    "            # Aplica suavização temporal\n",
    "            smoothed_scores = self._smooth_emotions(face_id, emotion_scores)\n",
    "            \n",
    "            dominant = max(smoothed_scores, key=smoothed_scores.get)\n",
    "            confidence = smoothed_scores[dominant]\n",
    "            \n",
    "            return EmotionResult(\n",
    "                face_id=face_id,\n",
    "                dominant_emotion=dominant,\n",
    "                emotion_scores=smoothed_scores,\n",
    "                confidence=confidence,\n",
    "                emotion_pt=EMOTION_LABELS.get(dominant, dominant)\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"[ERRO] DeepFace: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _analyze_simple(\n",
    "        self, \n",
    "        face_roi: np.ndarray, \n",
    "        face_id: int\n",
    "    ) -> EmotionResult:\n",
    "        \"\"\"\n",
    "        Análise simplificada baseada em características básicas.\n",
    "        Usado como fallback quando bibliotecas principais não estão disponíveis.\n",
    "        \"\"\"\n",
    "        # Análise muito básica baseada em brilho/contraste\n",
    "        gray = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)\n",
    "        mean_brightness = np.mean(gray) / 255.0\n",
    "        std_contrast = np.std(gray) / 128.0\n",
    "        \n",
    "        # Heurística simples (não é precisa, apenas para demonstração)\n",
    "        if std_contrast > 0.5:\n",
    "            dominant = \"surprise\" if mean_brightness > 0.5 else \"angry\"\n",
    "        elif mean_brightness > 0.6:\n",
    "            dominant = \"happy\"\n",
    "        elif mean_brightness < 0.4:\n",
    "            dominant = \"sad\"\n",
    "        else:\n",
    "            dominant = \"neutral\"\n",
    "        \n",
    "        emotion_scores = {\n",
    "            \"angry\": 0.1,\n",
    "            \"disgust\": 0.05,\n",
    "            \"fear\": 0.1,\n",
    "            \"happy\": 0.1,\n",
    "            \"sad\": 0.1,\n",
    "            \"surprise\": 0.1,\n",
    "            \"neutral\": 0.45\n",
    "        }\n",
    "        emotion_scores[dominant] = 0.6\n",
    "        \n",
    "        return EmotionResult(\n",
    "            face_id=face_id,\n",
    "            dominant_emotion=dominant,\n",
    "            emotion_scores=emotion_scores,\n",
    "            confidence=0.6,\n",
    "            emotion_pt=EMOTION_LABELS.get(dominant, dominant)\n",
    "        )\n",
    "    \n",
    "    def _smooth_emotions(\n",
    "        self, \n",
    "        face_id: int, \n",
    "        current_scores: Dict[str, float]\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Aplica suavização temporal nas emoções para reduzir ruído.\n",
    "        \n",
    "        Args:\n",
    "            face_id: ID do rosto\n",
    "            current_scores: Scores atuais\n",
    "            \n",
    "        Returns:\n",
    "            Scores suavizados\n",
    "        \"\"\"\n",
    "        # Inicializa histórico se necessário\n",
    "        if face_id not in self.emotion_history:\n",
    "            self.emotion_history[face_id] = deque(maxlen=self.temporal_window)\n",
    "        \n",
    "        self.emotion_history[face_id].append(current_scores)\n",
    "        \n",
    "        # Média ponderada (mais recentes têm mais peso)\n",
    "        if len(self.emotion_history[face_id]) == 1:\n",
    "            return current_scores\n",
    "        \n",
    "        smoothed = {}\n",
    "        weights = np.linspace(0.5, 1.0, len(self.emotion_history[face_id]))\n",
    "        weights /= weights.sum()\n",
    "        \n",
    "        for emotion in current_scores.keys():\n",
    "            values = [h.get(emotion, 0) for h in self.emotion_history[face_id]]\n",
    "            smoothed[emotion] = float(np.average(values, weights=weights))\n",
    "        \n",
    "        return smoothed\n",
    "    \n",
    "    def get_emotion_trend(self, face_id: int) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Analisa a tendência emocional de um rosto.\n",
    "        \n",
    "        Returns:\n",
    "            'increasing', 'decreasing', 'stable' ou None\n",
    "        \"\"\"\n",
    "        if face_id not in self.emotion_history:\n",
    "            return None\n",
    "        \n",
    "        history = list(self.emotion_history[face_id])\n",
    "        if len(history) < 3:\n",
    "            return None\n",
    "        \n",
    "        # Analisa tendência da emoção dominante\n",
    "        dominant_values = []\n",
    "        for h in history:\n",
    "            dominant = max(h, key=h.get)\n",
    "            dominant_values.append(h[dominant])\n",
    "        \n",
    "        trend = np.polyfit(range(len(dominant_values)), dominant_values, 1)[0]\n",
    "        \n",
    "        if trend > 0.05:\n",
    "            return \"increasing\"\n",
    "        elif trend < -0.05:\n",
    "            return \"decreasing\"\n",
    "        return \"stable\"\n",
    "    \n",
    "    def reset_history(self, face_id: Optional[int] = None):\n",
    "        \"\"\"Reseta o histórico de emoções.\"\"\"\n",
    "        if face_id is not None:\n",
    "            self.emotion_history.pop(face_id, None)\n",
    "        else:\n",
    "            self.emotion_history.clear()\n",
    "    \n",
    "    def draw_emotion(\n",
    "        self,\n",
    "        frame: np.ndarray,\n",
    "        bbox: Tuple[int, int, int, int],\n",
    "        result: EmotionResult,\n",
    "        color: Tuple[int, int, int] = (255, 255, 0)\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Desenha a emoção detectada no frame.\n",
    "        \n",
    "        Args:\n",
    "            frame: Imagem BGR\n",
    "            bbox: Bounding box do rosto\n",
    "            result: Resultado da análise\n",
    "            color: Cor do texto (BGR)\n",
    "            \n",
    "        Returns:\n",
    "            Frame anotado\n",
    "        \"\"\"\n",
    "        annotated = frame.copy()\n",
    "        x, y, w, h = bbox\n",
    "        \n",
    "        # Label da emoção\n",
    "        label = f\"{result.emotion_pt} ({result.confidence:.0%})\"\n",
    "        \n",
    "        # Posição abaixo do rosto\n",
    "        text_y = y + h + 20\n",
    "        \n",
    "        # Fundo do texto\n",
    "        (text_w, text_h), _ = cv2.getTextSize(\n",
    "            label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1\n",
    "        )\n",
    "        cv2.rectangle(\n",
    "            annotated,\n",
    "            (x, text_y - text_h - 5),\n",
    "            (x + text_w + 4, text_y + 5),\n",
    "            (0, 0, 0),\n",
    "            -1\n",
    "        )\n",
    "        \n",
    "        # Texto\n",
    "        cv2.putText(\n",
    "            annotated, label,\n",
    "            (x + 2, text_y),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.6,\n",
    "            color, 1, cv2.LINE_AA\n",
    "        )\n",
    "        \n",
    "        # Barra de emoções (mini gráfico)\n",
    "        bar_y = text_y + 15\n",
    "        bar_height = 8\n",
    "        bar_width = w\n",
    "        \n",
    "        for i, (emotion, score) in enumerate(sorted(\n",
    "            result.emotion_scores.items(), \n",
    "            key=lambda x: -x[1]\n",
    "        )[:3]):  # Top 3 emoções\n",
    "            bar_x = x\n",
    "            filled_width = int(bar_width * score)\n",
    "            \n",
    "            # Cores diferentes para cada emoção\n",
    "            emotion_colors = {\n",
    "                \"happy\": (0, 255, 0),\n",
    "                \"sad\": (255, 0, 0),\n",
    "                \"angry\": (0, 0, 255),\n",
    "                \"surprise\": (0, 255, 255),\n",
    "                \"fear\": (255, 0, 255),\n",
    "                \"disgust\": (0, 128, 0),\n",
    "                \"neutral\": (128, 128, 128)\n",
    "            }\n",
    "            bar_color = emotion_colors.get(emotion, (200, 200, 200))\n",
    "            \n",
    "            cv2.rectangle(\n",
    "                annotated,\n",
    "                (bar_x, bar_y + i * (bar_height + 2)),\n",
    "                (bar_x + filled_width, bar_y + i * (bar_height + 2) + bar_height),\n",
    "                bar_color,\n",
    "                -1\n",
    "            )\n",
    "        \n",
    "        return annotated\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /content/TC-4/src/activity_detector.py\n",
    "\"\"\"\n",
    "Tech Challenge - Fase 4: Detector de Atividades\n",
    "Usa YOLOv8-pose para detecção de pessoas e análise de poses/atividades.\n",
    "\"\"\"\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Importa torch após numpy para evitar bug de compatibilidade\n",
    "import torch\n",
    "\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from collections import deque\n",
    "from enum import Enum\n",
    "\n",
    "from .config import ACTIVITY_CATEGORIES\n",
    "\n",
    "\n",
    "class ActivityType(Enum):\n",
    "    \"\"\"Tipos de atividades detectáveis.\"\"\"\n",
    "    STANDING = \"standing\"\n",
    "    SITTING = \"sitting\"\n",
    "    WALKING = \"walking\"\n",
    "    RUNNING = \"running\"\n",
    "    WAVING = \"waving\"\n",
    "    POINTING = \"pointing\"\n",
    "    DANCING = \"dancing\"\n",
    "    CROUCHING = \"crouching\"\n",
    "    ARMS_RAISED = \"arms_raised\"\n",
    "    UNKNOWN = \"unknown\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PoseKeypoints:\n",
    "    \"\"\"Keypoints de pose (formato COCO - 17 pontos).\"\"\"\n",
    "    nose: Optional[Tuple[float, float]] = None\n",
    "    left_eye: Optional[Tuple[float, float]] = None\n",
    "    right_eye: Optional[Tuple[float, float]] = None\n",
    "    left_ear: Optional[Tuple[float, float]] = None\n",
    "    right_ear: Optional[Tuple[float, float]] = None\n",
    "    left_shoulder: Optional[Tuple[float, float]] = None\n",
    "    right_shoulder: Optional[Tuple[float, float]] = None\n",
    "    left_elbow: Optional[Tuple[float, float]] = None\n",
    "    right_elbow: Optional[Tuple[float, float]] = None\n",
    "    left_wrist: Optional[Tuple[float, float]] = None\n",
    "    right_wrist: Optional[Tuple[float, float]] = None\n",
    "    left_hip: Optional[Tuple[float, float]] = None\n",
    "    right_hip: Optional[Tuple[float, float]] = None\n",
    "    left_knee: Optional[Tuple[float, float]] = None\n",
    "    right_knee: Optional[Tuple[float, float]] = None\n",
    "    left_ankle: Optional[Tuple[float, float]] = None\n",
    "    right_ankle: Optional[Tuple[float, float]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ActivityDetection:\n",
    "    \"\"\"Resultado da detecção de atividade.\"\"\"\n",
    "    person_id: int\n",
    "    activity: ActivityType\n",
    "    activity_pt: str\n",
    "    confidence: float\n",
    "    bbox: Tuple[int, int, int, int]\n",
    "    keypoints: Optional[PoseKeypoints] = None\n",
    "    velocity: float = 0.0\n",
    "\n",
    "\n",
    "class ActivityDetector:\n",
    "    \"\"\"Detector de atividades usando YOLOv8-pose.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        model_size: str = \"n\",\n",
    "        min_confidence: float = 0.5,\n",
    "        history_size: int = 10\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_size: Tamanho do modelo ('n', 's', 'm', 'l', 'x')\n",
    "            min_confidence: Confiança mínima para detecção\n",
    "            history_size: Frames de histórico para análise temporal\n",
    "        \"\"\"\n",
    "        self.min_confidence = min_confidence\n",
    "        self.history_size = history_size\n",
    "        self.person_counter = 0\n",
    "        self.position_history: Dict[int, deque] = {}\n",
    "        self.pose_history: Dict[int, deque] = {}\n",
    "        \n",
    "        self._init_yolo(model_size)\n",
    "    \n",
    "    def _init_yolo(self, model_size: str):\n",
    "        \"\"\"Inicializa YOLO11-pose (mais preciso que YOLOv8).\"\"\"\n",
    "        try:\n",
    "            from ultralytics import YOLO\n",
    "            # Usa YOLO11 (melhor precisão)\n",
    "            model_name = f\"yolo11{model_size}-pose.pt\"\n",
    "            self.model = YOLO(model_name)\n",
    "            self.model_loaded = True\n",
    "            print(f\"[INFO] Modelo carregado: {model_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[AVISO] YOLO11 não disponível, tentando YOLOv8: {e}\")\n",
    "            try:\n",
    "                from ultralytics import YOLO\n",
    "                model_name = f\"yolov8{model_size}-pose.pt\"\n",
    "                self.model = YOLO(model_name)\n",
    "                self.model_loaded = True\n",
    "            except Exception as e2:\n",
    "                print(f\"[ERRO] Falha ao carregar modelo: {e2}\")\n",
    "                self.model = None\n",
    "                self.model_loaded = False\n",
    "    \n",
    "    def detect(self, frame: np.ndarray) -> List[ActivityDetection]:\n",
    "        \"\"\"Detecta pessoas e suas atividades no frame.\"\"\"\n",
    "        if not self.model_loaded:\n",
    "            return []\n",
    "        \n",
    "        results = self.model(frame, verbose=False, conf=self.min_confidence)\n",
    "        detections = []\n",
    "        \n",
    "        for result in results:\n",
    "            if result.keypoints is None or result.boxes is None:\n",
    "                continue\n",
    "            \n",
    "            keypoints_data = result.keypoints.xy.cpu().numpy()\n",
    "            confidences = result.keypoints.conf.cpu().numpy() if result.keypoints.conf is not None else None\n",
    "            boxes = result.boxes\n",
    "            \n",
    "            for i, (kpts, box) in enumerate(zip(keypoints_data, boxes)):\n",
    "                # Filtra por confiança do box\n",
    "                box_conf = float(box.conf[0])\n",
    "                if box_conf < self.min_confidence:\n",
    "                    continue\n",
    "                \n",
    "                # Bounding box\n",
    "                xyxy = box.xyxy[0].cpu().numpy()\n",
    "                bbox = (\n",
    "                    int(xyxy[0]), int(xyxy[1]),\n",
    "                    int(xyxy[2] - xyxy[0]), int(xyxy[3] - xyxy[1])\n",
    "                )\n",
    "                \n",
    "                # Keypoints\n",
    "                kpt_conf = confidences[i] if confidences is not None else None\n",
    "                keypoints = self._extract_keypoints(kpts, kpt_conf)\n",
    "                \n",
    "                # ID da pessoa (tracking simples por posição)\n",
    "                person_id = self._assign_person_id(bbox)\n",
    "                \n",
    "                # Analisa atividade\n",
    "                activity, act_conf = self._analyze_activity(person_id, keypoints)\n",
    "                \n",
    "                # Velocidade\n",
    "                velocity = self._calculate_velocity(person_id, bbox)\n",
    "                \n",
    "                detections.append(ActivityDetection(\n",
    "                    person_id=person_id,\n",
    "                    activity=activity,\n",
    "                    activity_pt=ACTIVITY_CATEGORIES.get(activity.value, \"Desconhecido\"),\n",
    "                    confidence=act_conf * box_conf,\n",
    "                    bbox=bbox,\n",
    "                    keypoints=keypoints,\n",
    "                    velocity=velocity\n",
    "                ))\n",
    "        \n",
    "        return detections\n",
    "    \n",
    "    def _extract_keypoints(\n",
    "        self, \n",
    "        kpts: np.ndarray, \n",
    "        conf: Optional[np.ndarray] = None,\n",
    "        min_kpt_conf: float = 0.3\n",
    "    ) -> PoseKeypoints:\n",
    "        \"\"\"Extrai keypoints do formato YOLO (COCO 17 pontos).\"\"\"\n",
    "        def get_point(idx: int) -> Optional[Tuple[float, float]]:\n",
    "            if idx >= len(kpts):\n",
    "                return None\n",
    "            x, y = kpts[idx]\n",
    "            if x == 0 and y == 0:\n",
    "                return None\n",
    "            if conf is not None and idx < len(conf) and conf[idx] < min_kpt_conf:\n",
    "                return None\n",
    "            return (float(x), float(y))\n",
    "        \n",
    "        return PoseKeypoints(\n",
    "            nose=get_point(0),\n",
    "            left_eye=get_point(1),\n",
    "            right_eye=get_point(2),\n",
    "            left_ear=get_point(3),\n",
    "            right_ear=get_point(4),\n",
    "            left_shoulder=get_point(5),\n",
    "            right_shoulder=get_point(6),\n",
    "            left_elbow=get_point(7),\n",
    "            right_elbow=get_point(8),\n",
    "            left_wrist=get_point(9),\n",
    "            right_wrist=get_point(10),\n",
    "            left_hip=get_point(11),\n",
    "            right_hip=get_point(12),\n",
    "            left_knee=get_point(13),\n",
    "            right_knee=get_point(14),\n",
    "            left_ankle=get_point(15),\n",
    "            right_ankle=get_point(16)\n",
    "        )\n",
    "    \n",
    "    def _assign_person_id(self, bbox: Tuple[int, int, int, int]) -> int:\n",
    "        \"\"\"Atribui ID baseado em proximidade com detecções anteriores.\"\"\"\n",
    "        cx = bbox[0] + bbox[2] // 2\n",
    "        cy = bbox[1] + bbox[3] // 2\n",
    "        \n",
    "        # Busca pessoa próxima no histórico\n",
    "        min_dist = float('inf')\n",
    "        best_id = None\n",
    "        \n",
    "        for pid, history in self.position_history.items():\n",
    "            if history:\n",
    "                last_pos = history[-1]\n",
    "                dist = np.sqrt((cx - last_pos[0])**2 + (cy - last_pos[1])**2)\n",
    "                if dist < min_dist and dist < 100:\n",
    "                    min_dist = dist\n",
    "                    best_id = pid\n",
    "        \n",
    "        if best_id is None:\n",
    "            self.person_counter += 1\n",
    "            best_id = self.person_counter\n",
    "            self.position_history[best_id] = deque(maxlen=self.history_size)\n",
    "            self.pose_history[best_id] = deque(maxlen=self.history_size)\n",
    "        \n",
    "        self.position_history[best_id].append((cx, cy))\n",
    "        return best_id\n",
    "    \n",
    "    def _calculate_velocity(self, person_id: int, bbox: Tuple[int, int, int, int]) -> float:\n",
    "        \"\"\"Calcula velocidade do movimento em pixels/frame.\"\"\"\n",
    "        history = self.position_history.get(person_id)\n",
    "        if not history or len(history) < 2:\n",
    "            return 0.0\n",
    "        \n",
    "        positions = list(history)\n",
    "        if len(positions) >= 2:\n",
    "            dx = positions[-1][0] - positions[-2][0]\n",
    "            dy = positions[-1][1] - positions[-2][1]\n",
    "            return np.sqrt(dx**2 + dy**2)\n",
    "        return 0.0\n",
    "    \n",
    "    def _analyze_activity(\n",
    "        self, \n",
    "        person_id: int, \n",
    "        keypoints: PoseKeypoints\n",
    "    ) -> Tuple[ActivityType, float]:\n",
    "        \"\"\"Analisa atividade baseada em pose e histórico.\"\"\"\n",
    "        self.pose_history.setdefault(person_id, deque(maxlen=self.history_size))\n",
    "        self.pose_history[person_id].append(keypoints)\n",
    "        \n",
    "        # 1. Verifica braços levantados (ambos acima da cabeça)\n",
    "        if self._is_arms_raised(keypoints):\n",
    "            return ActivityType.ARMS_RAISED, 0.85\n",
    "        \n",
    "        # 2. Verifica se está dançando (movimento rítmico + gestos)\n",
    "        if self._is_dancing(person_id, keypoints):\n",
    "            return ActivityType.DANCING, 0.8\n",
    "        \n",
    "        # 3. Verifica gestos com braços\n",
    "        if self._is_waving(keypoints):\n",
    "            return ActivityType.WAVING, 0.8\n",
    "        \n",
    "        if self._is_pointing(keypoints):\n",
    "            return ActivityType.POINTING, 0.75\n",
    "        \n",
    "        # 4. Verifica agachado\n",
    "        if self._is_crouching(keypoints):\n",
    "            return ActivityType.CROUCHING, 0.75\n",
    "        \n",
    "        # 5. Verifica postura sentada (melhorada)\n",
    "        if self._is_sitting(keypoints):\n",
    "            return ActivityType.SITTING, 0.75\n",
    "        \n",
    "        # 6. Verifica movimento pelas pernas e velocidade\n",
    "        velocity = self._get_avg_velocity(person_id)\n",
    "        \n",
    "        if velocity > 80:\n",
    "            return ActivityType.RUNNING, 0.8\n",
    "        elif velocity > 25:\n",
    "            return ActivityType.WALKING, 0.75\n",
    "        else:\n",
    "            return ActivityType.STANDING, 0.7\n",
    "    \n",
    "    def _get_avg_velocity(self, person_id: int) -> float:\n",
    "        \"\"\"Calcula velocidade média recente.\"\"\"\n",
    "        history = self.position_history.get(person_id)\n",
    "        if not history or len(history) < 3:\n",
    "            return 0.0\n",
    "        \n",
    "        positions = list(history)\n",
    "        velocities = []\n",
    "        for i in range(1, len(positions)):\n",
    "            dx = positions[i][0] - positions[i-1][0]\n",
    "            dy = positions[i][1] - positions[i-1][1]\n",
    "            velocities.append(np.sqrt(dx**2 + dy**2))\n",
    "        \n",
    "        return np.mean(velocities) if velocities else 0.0\n",
    "    \n",
    "    def _is_waving(self, kp: PoseKeypoints) -> bool:\n",
    "        \"\"\"Detecta gesto de acenar (mão acima do ombro, cotovelo dobrado).\"\"\"\n",
    "        for wrist, elbow, shoulder in [\n",
    "            (kp.left_wrist, kp.left_elbow, kp.left_shoulder),\n",
    "            (kp.right_wrist, kp.right_elbow, kp.right_shoulder)\n",
    "        ]:\n",
    "            if all([wrist, elbow, shoulder]):\n",
    "                # Mão acima do ombro\n",
    "                if wrist[1] < shoulder[1] - 30:\n",
    "                    # Cotovelo dobrado (não braço reto)\n",
    "                    elbow_angle = self._calculate_angle(shoulder, elbow, wrist)\n",
    "                    if 45 < elbow_angle < 150:\n",
    "                        return True\n",
    "        return False\n",
    "    \n",
    "    def _is_pointing(self, kp: PoseKeypoints) -> bool:\n",
    "        \"\"\"Detecta gesto de apontar (braço estendido horizontalmente).\"\"\"\n",
    "        for wrist, elbow, shoulder in [\n",
    "            (kp.left_wrist, kp.left_elbow, kp.left_shoulder),\n",
    "            (kp.right_wrist, kp.right_elbow, kp.right_shoulder)\n",
    "        ]:\n",
    "            if all([wrist, elbow, shoulder]):\n",
    "                # Braço estendido (ângulo > 150)\n",
    "                arm_angle = self._calculate_angle(shoulder, elbow, wrist)\n",
    "                if arm_angle > 150:\n",
    "                    # Horizontalmente (variação vertical pequena)\n",
    "                    arm_height_diff = abs(wrist[1] - shoulder[1])\n",
    "                    arm_length = abs(wrist[0] - shoulder[0])\n",
    "                    if arm_length > 80 and arm_height_diff < 60:\n",
    "                        return True\n",
    "        return False\n",
    "    \n",
    "    def _is_sitting(self, kp: PoseKeypoints) -> bool:\n",
    "        \"\"\"Detecta postura sentada (análise melhorada de ângulos).\"\"\"\n",
    "        # Precisa de quadril e joelho\n",
    "        if not all([kp.left_hip, kp.right_hip, kp.left_knee, kp.right_knee]):\n",
    "            return False\n",
    "        \n",
    "        hip_y = (kp.left_hip[1] + kp.right_hip[1]) / 2\n",
    "        knee_y = (kp.left_knee[1] + kp.right_knee[1]) / 2\n",
    "        \n",
    "        # Se joelhos estão aproximadamente na mesma altura que quadril = sentado\n",
    "        hip_knee_diff = abs(hip_y - knee_y)\n",
    "        \n",
    "        # Verifica também ângulo do tronco se tiver ombros\n",
    "        if kp.left_shoulder and kp.right_shoulder:\n",
    "            shoulder_y = (kp.left_shoulder[1] + kp.right_shoulder[1]) / 2\n",
    "            torso_length = abs(hip_y - shoulder_y)\n",
    "            \n",
    "            # Se diferença quadril-joelho é menor que 40% do tronco = sentado\n",
    "            if hip_knee_diff < torso_length * 0.5:\n",
    "                return True\n",
    "        \n",
    "        # Fallback: diferença absoluta pequena\n",
    "        if hip_knee_diff < 80:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def _is_crouching(self, kp: PoseKeypoints) -> bool:\n",
    "        \"\"\"Detecta postura agachada (quadril baixo, joelhos dobrados).\"\"\"\n",
    "        if not all([kp.left_hip, kp.right_hip, kp.left_knee, kp.right_knee]):\n",
    "            return False\n",
    "        \n",
    "        hip_y = (kp.left_hip[1] + kp.right_hip[1]) / 2\n",
    "        knee_y = (kp.left_knee[1] + kp.right_knee[1]) / 2\n",
    "        \n",
    "        # Tornozelos disponíveis para melhor análise\n",
    "        if kp.left_ankle and kp.right_ankle:\n",
    "            ankle_y = (kp.left_ankle[1] + kp.right_ankle[1]) / 2\n",
    "            \n",
    "            # Agachado: quadril próximo aos joelhos, joelhos acima dos tornozelos\n",
    "            if hip_y > knee_y - 30 and knee_y < ankle_y:\n",
    "                return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def _is_arms_raised(self, kp: PoseKeypoints) -> bool:\n",
    "        \"\"\"Detecta ambos os braços levantados acima da cabeça.\"\"\"\n",
    "        if not all([kp.left_wrist, kp.right_wrist, kp.nose]):\n",
    "            return False\n",
    "        \n",
    "        # Ambos pulsos acima do nariz\n",
    "        if kp.left_wrist[1] < kp.nose[1] and kp.right_wrist[1] < kp.nose[1]:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def _is_dancing(self, person_id: int, kp: PoseKeypoints) -> bool:\n",
    "        \"\"\"Detecta dança baseada em movimento rítmico e variação de pose.\"\"\"\n",
    "        history = self.pose_history.get(person_id)\n",
    "        if not history or len(history) < 5:\n",
    "            return False\n",
    "        \n",
    "        # Calcula variação de posição dos braços ao longo do tempo\n",
    "        wrist_variations = []\n",
    "        for prev_kp in list(history)[-5:]:\n",
    "            if prev_kp.left_wrist and kp.left_wrist:\n",
    "                dx = abs(prev_kp.left_wrist[0] - kp.left_wrist[0])\n",
    "                dy = abs(prev_kp.left_wrist[1] - kp.left_wrist[1])\n",
    "                wrist_variations.append(dx + dy)\n",
    "            if prev_kp.right_wrist and kp.right_wrist:\n",
    "                dx = abs(prev_kp.right_wrist[0] - kp.right_wrist[0])\n",
    "                dy = abs(prev_kp.right_wrist[1] - kp.right_wrist[1])\n",
    "                wrist_variations.append(dx + dy)\n",
    "        \n",
    "        # Se há movimento constante dos braços = possível dança\n",
    "        if wrist_variations:\n",
    "            avg_variation = np.mean(wrist_variations)\n",
    "            # Movimento moderado (não parado, não muito rápido)\n",
    "            if 15 < avg_variation < 80:\n",
    "                # Verifica se torso também se move\n",
    "                velocity = self._get_avg_velocity(person_id)\n",
    "                if 5 < velocity < 40:\n",
    "                    return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def _calculate_angle(\n",
    "        self, \n",
    "        p1: Tuple[float, float], \n",
    "        p2: Tuple[float, float], \n",
    "        p3: Tuple[float, float]\n",
    "    ) -> float:\n",
    "        \"\"\"Calcula ângulo entre três pontos (p2 é o vértice).\"\"\"\n",
    "        v1 = np.array([p1[0] - p2[0], p1[1] - p2[1]])\n",
    "        v2 = np.array([p3[0] - p2[0], p3[1] - p2[1]])\n",
    "        \n",
    "        cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-6)\n",
    "        angle = np.arccos(np.clip(cos_angle, -1, 1))\n",
    "        \n",
    "        return np.degrees(angle)\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reseta estado do detector.\"\"\"\n",
    "        self.person_counter = 0\n",
    "        self.position_history.clear()\n",
    "        self.pose_history.clear()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /content/TC-4/src/anomaly_detector.py\n",
    "\"\"\"\n",
    "Tech Challenge - Fase 4: Detector de Anomalias\n",
    "Módulo responsável pela detecção de comportamentos anômalos no vídeo.\n",
    "Anomalias incluem: movimentos bruscos, mudanças emocionais súbitas, padrões atípicos.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass, field\n",
    "from collections import deque\n",
    "from enum import Enum\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class AnomalyType(Enum):\n",
    "    \"\"\"Tipos de anomalias detectáveis.\"\"\"\n",
    "    SUDDEN_MOVEMENT = \"sudden_movement\"\n",
    "    EMOTION_SPIKE = \"emotion_spike\"\n",
    "    UNUSUAL_ACTIVITY = \"unusual_activity\"\n",
    "    CROWD_ANOMALY = \"crowd_anomaly\"\n",
    "    PROLONGED_INACTIVITY = \"prolonged_inactivity\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AnomalyEvent:\n",
    "    \"\"\"Representa um evento anômalo detectado.\"\"\"\n",
    "    anomaly_type: AnomalyType\n",
    "    timestamp: float  # Segundos desde o início do vídeo\n",
    "    frame_number: int\n",
    "    person_id: Optional[int]\n",
    "    severity: float  # 0.0 a 1.0\n",
    "    description: str\n",
    "    bbox: Optional[Tuple[int, int, int, int]] = None\n",
    "    details: Dict = field(default_factory=dict)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PersonMetrics:\n",
    "    \"\"\"Métricas acumuladas de uma pessoa para análise de anomalias.\"\"\"\n",
    "    position_history: deque = field(default_factory=lambda: deque(maxlen=30))\n",
    "    emotion_history: deque = field(default_factory=lambda: deque(maxlen=30))\n",
    "    activity_history: deque = field(default_factory=lambda: deque(maxlen=30))\n",
    "    velocity_history: deque = field(default_factory=lambda: deque(maxlen=30))\n",
    "    last_seen_frame: int = 0\n",
    "    frames_inactive: int = 0\n",
    "\n",
    "\n",
    "class AnomalyDetector:\n",
    "    \"\"\"\n",
    "    Detector de anomalias comportamentais.\n",
    "    Analisa padrões de movimento, emoção e atividade para identificar comportamentos atípicos.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        sudden_movement_threshold: float = 80.0,\n",
    "        emotion_change_threshold: float = 0.5,\n",
    "        inactivity_threshold: int = 90,  # frames\n",
    "        fps: float = 30.0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Inicializa o detector de anomalias.\n",
    "        \n",
    "        Args:\n",
    "            sudden_movement_threshold: Limiar de velocidade para movimento brusco (pixels/frame)\n",
    "            emotion_change_threshold: Limiar de mudança emocional (0-1)\n",
    "            inactivity_threshold: Frames de inatividade para considerar anomalia\n",
    "            fps: Frames por segundo do vídeo\n",
    "        \"\"\"\n",
    "        self.sudden_movement_threshold = sudden_movement_threshold\n",
    "        self.emotion_change_threshold = emotion_change_threshold\n",
    "        self.inactivity_threshold = inactivity_threshold\n",
    "        self.fps = fps\n",
    "        \n",
    "        # Métricas por pessoa\n",
    "        self.person_metrics: Dict[int, PersonMetrics] = {}\n",
    "        \n",
    "        # Histórico de anomalias\n",
    "        self.anomaly_history: List[AnomalyEvent] = []\n",
    "        \n",
    "        # Estatísticas globais para baseline\n",
    "        self.global_velocity_mean = 0.0\n",
    "        self.global_velocity_std = 1.0\n",
    "        self.velocity_samples: deque = deque(maxlen=1000)\n",
    "        \n",
    "        # Contadores\n",
    "        self.frame_count = 0\n",
    "        self.total_detections = 0\n",
    "    \n",
    "    def update(\n",
    "        self,\n",
    "        frame_number: int,\n",
    "        face_detections: List,\n",
    "        emotion_results: List,\n",
    "        activity_detections: List\n",
    "    ) -> List[AnomalyEvent]:\n",
    "        \"\"\"\n",
    "        Atualiza o detector com novos dados e retorna anomalias detectadas.\n",
    "        \n",
    "        Args:\n",
    "            frame_number: Número do frame atual\n",
    "            face_detections: Lista de detecções de rostos\n",
    "            emotion_results: Lista de resultados de análise emocional\n",
    "            activity_detections: Lista de atividades detectadas\n",
    "            \n",
    "        Returns:\n",
    "            Lista de anomalias detectadas neste frame\n",
    "        \"\"\"\n",
    "        self.frame_count = frame_number\n",
    "        anomalies = []\n",
    "        \n",
    "        # Atualiza métricas de cada pessoa\n",
    "        seen_persons = set()\n",
    "        \n",
    "        # Processa detecções de face/emoção\n",
    "        for face, emotion in zip(face_detections, emotion_results):\n",
    "            if emotion is None:\n",
    "                continue\n",
    "            \n",
    "            person_id = face.face_id\n",
    "            seen_persons.add(person_id)\n",
    "            \n",
    "            self._ensure_person_metrics(person_id)\n",
    "            metrics = self.person_metrics[person_id]\n",
    "            \n",
    "            # Atualiza histórico\n",
    "            center = np.array([\n",
    "                face.bbox[0] + face.bbox[2]/2,\n",
    "                face.bbox[1] + face.bbox[3]/2\n",
    "            ])\n",
    "            metrics.position_history.append(center)\n",
    "            metrics.emotion_history.append(emotion.emotion_scores.copy())\n",
    "            metrics.last_seen_frame = frame_number\n",
    "            metrics.frames_inactive = 0\n",
    "            \n",
    "            # Detecta anomalias de emoção\n",
    "            emotion_anomaly = self._check_emotion_anomaly(person_id, emotion, frame_number)\n",
    "            if emotion_anomaly:\n",
    "                emotion_anomaly.bbox = face.bbox\n",
    "                anomalies.append(emotion_anomaly)\n",
    "        \n",
    "        # Processa detecções de atividade\n",
    "        for activity in activity_detections:\n",
    "            person_id = activity.person_id\n",
    "            seen_persons.add(person_id)\n",
    "            \n",
    "            self._ensure_person_metrics(person_id)\n",
    "            metrics = self.person_metrics[person_id]\n",
    "            \n",
    "            # Atualiza histórico\n",
    "            metrics.activity_history.append(activity.activity.value)\n",
    "            metrics.velocity_history.append(activity.velocity)\n",
    "            \n",
    "            # Atualiza estatísticas globais\n",
    "            self.velocity_samples.append(activity.velocity)\n",
    "            if len(self.velocity_samples) > 100:\n",
    "                self.global_velocity_mean = np.mean(self.velocity_samples)\n",
    "                self.global_velocity_std = max(np.std(self.velocity_samples), 1.0)\n",
    "            \n",
    "            # Detecta anomalias de movimento\n",
    "            movement_anomaly = self._check_movement_anomaly(person_id, activity, frame_number)\n",
    "            if movement_anomaly:\n",
    "                anomalies.append(movement_anomaly)\n",
    "            \n",
    "            # Detecta anomalias de atividade\n",
    "            activity_anomaly = self._check_activity_anomaly(person_id, activity, frame_number)\n",
    "            if activity_anomaly:\n",
    "                anomalies.append(activity_anomaly)\n",
    "        \n",
    "        # Verifica inatividade prolongada\n",
    "        for person_id, metrics in self.person_metrics.items():\n",
    "            if person_id not in seen_persons:\n",
    "                metrics.frames_inactive += 1\n",
    "                \n",
    "                if metrics.frames_inactive == self.inactivity_threshold:\n",
    "                    anomalies.append(AnomalyEvent(\n",
    "                        anomaly_type=AnomalyType.PROLONGED_INACTIVITY,\n",
    "                        timestamp=frame_number / self.fps,\n",
    "                        frame_number=frame_number,\n",
    "                        person_id=person_id,\n",
    "                        severity=0.4,\n",
    "                        description=f\"Pessoa #{person_id} desapareceu por {self.inactivity_threshold} frames\"\n",
    "                    ))\n",
    "        \n",
    "        # Registra anomalias no histórico\n",
    "        self.anomaly_history.extend(anomalies)\n",
    "        self.total_detections += len(face_detections) + len(activity_detections)\n",
    "        \n",
    "        return anomalies\n",
    "    \n",
    "    def _ensure_person_metrics(self, person_id: int):\n",
    "        \"\"\"Garante que métricas existam para a pessoa.\"\"\"\n",
    "        if person_id not in self.person_metrics:\n",
    "            self.person_metrics[person_id] = PersonMetrics()\n",
    "    \n",
    "    def _check_emotion_anomaly(\n",
    "        self,\n",
    "        person_id: int,\n",
    "        emotion_result,\n",
    "        frame_number: int\n",
    "    ) -> Optional[AnomalyEvent]:\n",
    "        \"\"\"Verifica anomalias de mudança emocional.\"\"\"\n",
    "        metrics = self.person_metrics[person_id]\n",
    "        history = metrics.emotion_history\n",
    "        \n",
    "        if len(history) < 3:\n",
    "            return None\n",
    "        \n",
    "        # Calcula mudança emocional\n",
    "        prev_scores = history[-2]\n",
    "        curr_scores = emotion_result.emotion_scores\n",
    "        \n",
    "        change = 0.0\n",
    "        for emotion in curr_scores:\n",
    "            prev_val = prev_scores.get(emotion, 0)\n",
    "            curr_val = curr_scores.get(emotion, 0)\n",
    "            change += abs(curr_val - prev_val)\n",
    "        \n",
    "        change /= len(curr_scores)\n",
    "        \n",
    "        if change > self.emotion_change_threshold:\n",
    "            # Identifica qual emoção mudou mais\n",
    "            max_change_emotion = max(\n",
    "                curr_scores.keys(),\n",
    "                key=lambda e: abs(curr_scores[e] - prev_scores.get(e, 0))\n",
    "            )\n",
    "            \n",
    "            return AnomalyEvent(\n",
    "                anomaly_type=AnomalyType.EMOTION_SPIKE,\n",
    "                timestamp=frame_number / self.fps,\n",
    "                frame_number=frame_number,\n",
    "                person_id=person_id,\n",
    "                severity=min(change / self.emotion_change_threshold, 1.0),\n",
    "                description=f\"Mudança emocional brusca para '{max_change_emotion}'\",\n",
    "                details={\n",
    "                    \"emotion\": max_change_emotion,\n",
    "                    \"change_magnitude\": change,\n",
    "                    \"previous_dominant\": max(prev_scores, key=prev_scores.get),\n",
    "                    \"current_dominant\": emotion_result.dominant_emotion\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _check_movement_anomaly(\n",
    "        self,\n",
    "        person_id: int,\n",
    "        activity_detection,\n",
    "        frame_number: int\n",
    "    ) -> Optional[AnomalyEvent]:\n",
    "        \"\"\"Verifica anomalias de movimento brusco.\"\"\"\n",
    "        velocity = activity_detection.velocity\n",
    "        \n",
    "        # Usa threshold absoluto e relativo\n",
    "        abs_threshold = self.sudden_movement_threshold\n",
    "        rel_threshold = self.global_velocity_mean + 3 * self.global_velocity_std\n",
    "        \n",
    "        threshold = max(abs_threshold, rel_threshold)\n",
    "        \n",
    "        if velocity > threshold:\n",
    "            severity = min((velocity - threshold) / threshold + 0.5, 1.0)\n",
    "            \n",
    "            return AnomalyEvent(\n",
    "                anomaly_type=AnomalyType.SUDDEN_MOVEMENT,\n",
    "                timestamp=frame_number / self.fps,\n",
    "                frame_number=frame_number,\n",
    "                person_id=person_id,\n",
    "                severity=severity,\n",
    "                description=f\"Movimento brusco detectado (velocidade: {velocity:.1f} px/frame)\",\n",
    "                bbox=activity_detection.bbox,\n",
    "                details={\n",
    "                    \"velocity\": velocity,\n",
    "                    \"threshold\": threshold,\n",
    "                    \"activity\": activity_detection.activity.value\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _check_activity_anomaly(\n",
    "        self,\n",
    "        person_id: int,\n",
    "        activity_detection,\n",
    "        frame_number: int\n",
    "    ) -> Optional[AnomalyEvent]:\n",
    "        \"\"\"Verifica anomalias de padrão de atividade.\"\"\"\n",
    "        metrics = self.person_metrics[person_id]\n",
    "        history = list(metrics.activity_history)\n",
    "        \n",
    "        if len(history) < 10:\n",
    "            return None\n",
    "        \n",
    "        current_activity = activity_detection.activity.value\n",
    "        \n",
    "        # Conta frequência de cada atividade no histórico\n",
    "        activity_counts = {}\n",
    "        for act in history[:-1]:  # Exclui o atual\n",
    "            activity_counts[act] = activity_counts.get(act, 0) + 1\n",
    "        \n",
    "        total = len(history) - 1\n",
    "        if total == 0:\n",
    "            return None\n",
    "        \n",
    "        # Calcula probabilidade da atividade atual baseado no histórico\n",
    "        current_freq = activity_counts.get(current_activity, 0) / total\n",
    "        \n",
    "        # Se atividade atual é muito rara no histórico (< 5%), é anômala\n",
    "        if current_freq < 0.05 and len(set(history)) > 2:\n",
    "            most_common = max(activity_counts, key=activity_counts.get)\n",
    "            \n",
    "            return AnomalyEvent(\n",
    "                anomaly_type=AnomalyType.UNUSUAL_ACTIVITY,\n",
    "                timestamp=frame_number / self.fps,\n",
    "                frame_number=frame_number,\n",
    "                person_id=person_id,\n",
    "                severity=0.5,\n",
    "                description=f\"Atividade incomum: '{current_activity}' (usual: '{most_common}')\",\n",
    "                bbox=activity_detection.bbox,\n",
    "                details={\n",
    "                    \"activity\": current_activity,\n",
    "                    \"frequency\": current_freq,\n",
    "                    \"usual_activity\": most_common\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def get_statistics(self) -> Dict:\n",
    "        \"\"\"Retorna estatísticas do detector.\"\"\"\n",
    "        anomaly_counts = {}\n",
    "        for anomaly in self.anomaly_history:\n",
    "            atype = anomaly.anomaly_type.value\n",
    "            anomaly_counts[atype] = anomaly_counts.get(atype, 0) + 1\n",
    "        \n",
    "        severity_avg = 0.0\n",
    "        if self.anomaly_history:\n",
    "            severity_avg = np.mean([a.severity for a in self.anomaly_history])\n",
    "        \n",
    "        return {\n",
    "            \"total_frames\": self.frame_count,\n",
    "            \"total_anomalies\": len(self.anomaly_history),\n",
    "            \"anomalies_by_type\": anomaly_counts,\n",
    "            \"average_severity\": severity_avg,\n",
    "            \"persons_tracked\": len(self.person_metrics),\n",
    "            \"global_velocity_mean\": self.global_velocity_mean,\n",
    "            \"global_velocity_std\": self.global_velocity_std\n",
    "        }\n",
    "    \n",
    "    def get_anomalies_summary(self) -> List[Dict]:\n",
    "        \"\"\"Retorna resumo das anomalias para relatório.\"\"\"\n",
    "        summary = []\n",
    "        for anomaly in self.anomaly_history:\n",
    "            summary.append({\n",
    "                \"tipo\": anomaly.anomaly_type.value,\n",
    "                \"timestamp\": f\"{anomaly.timestamp:.2f}s\",\n",
    "                \"frame\": anomaly.frame_number,\n",
    "                \"pessoa_id\": anomaly.person_id,\n",
    "                \"severidade\": f\"{anomaly.severity:.0%}\",\n",
    "                \"descricao\": anomaly.description,\n",
    "                \"detalhes\": anomaly.details\n",
    "            })\n",
    "        return summary\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reseta o estado do detector.\"\"\"\n",
    "        self.person_metrics.clear()\n",
    "        self.anomaly_history.clear()\n",
    "        self.velocity_samples.clear()\n",
    "        self.frame_count = 0\n",
    "        self.total_detections = 0\n",
    "        self.global_velocity_mean = 0.0\n",
    "        self.global_velocity_std = 1.0\n",
    "\n",
    "\n",
    "def draw_anomaly(\n",
    "    frame: np.ndarray,\n",
    "    anomaly: AnomalyEvent,\n",
    "    color: Tuple[int, int, int] = (0, 0, 255)\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Desenha indicação de anomalia no frame.\n",
    "    \n",
    "    Args:\n",
    "        frame: Imagem BGR\n",
    "        anomaly: Evento de anomalia\n",
    "        color: Cor do indicador (BGR)\n",
    "        \n",
    "    Returns:\n",
    "        Frame anotado\n",
    "    \"\"\"\n",
    "    import cv2\n",
    "    annotated = frame.copy()\n",
    "    h, w = frame.shape[:2]\n",
    "    \n",
    "    # Se tem bbox, destaca a região\n",
    "    if anomaly.bbox:\n",
    "        x, y, bw, bh = anomaly.bbox\n",
    "        \n",
    "        # Borda pulsante (mais grossa para severidade maior)\n",
    "        thickness = int(2 + anomaly.severity * 4)\n",
    "        cv2.rectangle(annotated, (x, y), (x+bw, y+bh), color, thickness)\n",
    "        \n",
    "        # Ícone de alerta\n",
    "        alert_x = x + bw - 25\n",
    "        alert_y = y + 5\n",
    "        cv2.putText(\n",
    "            annotated, \"!\",\n",
    "            (alert_x, alert_y + 20),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.8,\n",
    "            color, 2, cv2.LINE_AA\n",
    "        )\n",
    "    \n",
    "    # Banner de anomalia no topo\n",
    "    banner_text = f\"ANOMALIA: {anomaly.description}\"\n",
    "    (text_w, text_h), _ = cv2.getTextSize(\n",
    "        banner_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1\n",
    "    )\n",
    "    \n",
    "    # Fundo semi-transparente\n",
    "    overlay = annotated.copy()\n",
    "    cv2.rectangle(overlay, (0, 0), (w, text_h + 20), color, -1)\n",
    "    cv2.addWeighted(overlay, 0.7, annotated, 0.3, 0, annotated)\n",
    "    \n",
    "    # Texto\n",
    "    cv2.putText(\n",
    "        annotated, banner_text,\n",
    "        (10, text_h + 10),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 0.6,\n",
    "        (255, 255, 255), 1, cv2.LINE_AA\n",
    "    )\n",
    "    \n",
    "    return annotated\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /content/TC-4/src/report_generator.py\n",
    "\"\"\"\n",
    "Tech Challenge - Fase 4: Gerador de Relatório\n",
    "Módulo responsável pela geração automática do relatório de análise do vídeo.\n",
    "Suporta geração com LLM (OpenAI) ou template local.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "from typing import Dict, List, Optional\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from .config import OPENAI_API_KEY, OPENAI_MODEL, REPORTS_DIR, EMOTION_LABELS\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class VideoAnalysisResult:\n",
    "    \"\"\"Resultado consolidado da análise de vídeo.\"\"\"\n",
    "    video_path: str\n",
    "    total_frames: int\n",
    "    fps: float\n",
    "    duration_seconds: float\n",
    "    total_faces_detected: int\n",
    "    unique_faces: int\n",
    "    emotions_summary: Dict[str, int]\n",
    "    activities_summary: Dict[str, int]\n",
    "    total_anomalies: int\n",
    "    anomalies_by_type: Dict[str, int]\n",
    "    anomaly_events: List[Dict]\n",
    "    processing_time_seconds: float\n",
    "\n",
    "\n",
    "class ReportGenerator:\n",
    "    \"\"\"\n",
    "    Gerador de relatórios de análise de vídeo.\n",
    "    Suporta geração com LLM para resumos mais elaborados.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, use_llm: bool = True):\n",
    "        \"\"\"\n",
    "        Inicializa o gerador de relatórios.\n",
    "        \n",
    "        Args:\n",
    "            use_llm: Se True, usa LLM para gerar resumos (requer OPENAI_API_KEY)\n",
    "        \"\"\"\n",
    "        self.use_llm = use_llm and bool(OPENAI_API_KEY)\n",
    "        self.llm = None\n",
    "        \n",
    "        if self.use_llm:\n",
    "            self._init_llm()\n",
    "    \n",
    "    def _init_llm(self):\n",
    "        \"\"\"Inicializa o LLM para geração de resumos.\"\"\"\n",
    "        try:\n",
    "            from langchain_openai import ChatOpenAI\n",
    "            from langchain.prompts import PromptTemplate\n",
    "            \n",
    "            self.llm = ChatOpenAI(model=OPENAI_MODEL, temperature=0.3)\n",
    "            self.PromptTemplate = PromptTemplate\n",
    "        except ImportError:\n",
    "            print(\"[AVISO] LangChain não instalado, usando geração de relatório sem LLM\")\n",
    "            self.use_llm = False\n",
    "    \n",
    "    def generate(\n",
    "        self,\n",
    "        analysis_result: VideoAnalysisResult,\n",
    "        output_path: Optional[str] = None\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Gera o relatório de análise.\n",
    "        \n",
    "        Args:\n",
    "            analysis_result: Resultado da análise do vídeo\n",
    "            output_path: Caminho para salvar o relatório (opcional)\n",
    "            \n",
    "        Returns:\n",
    "            Texto do relatório gerado\n",
    "        \"\"\"\n",
    "        # Gera seções do relatório\n",
    "        header = self._generate_header(analysis_result)\n",
    "        statistics = self._generate_statistics(analysis_result)\n",
    "        emotions_section = self._generate_emotions_section(analysis_result)\n",
    "        activities_section = self._generate_activities_section(analysis_result)\n",
    "        anomalies_section = self._generate_anomalies_section(analysis_result)\n",
    "        \n",
    "        # Gera resumo executivo\n",
    "        if self.use_llm:\n",
    "            summary = self._generate_llm_summary(analysis_result)\n",
    "        else:\n",
    "            summary = self._generate_template_summary(analysis_result)\n",
    "        \n",
    "        # Monta relatório completo\n",
    "        report = f\"\"\"\n",
    "{header}\n",
    "\n",
    "## Resumo Executivo\n",
    "{summary}\n",
    "\n",
    "{statistics}\n",
    "\n",
    "{emotions_section}\n",
    "\n",
    "{activities_section}\n",
    "\n",
    "{anomalies_section}\n",
    "\n",
    "---\n",
    "*Relatório gerado automaticamente em {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}*\n",
    "\"\"\"\n",
    "        \n",
    "        # Salva se caminho fornecido\n",
    "        if output_path:\n",
    "            Path(output_path).write_text(report, encoding='utf-8')\n",
    "            print(f\"[INFO] Relatório salvo em: {output_path}\")\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def _generate_header(self, result: VideoAnalysisResult) -> str:\n",
    "        \"\"\"Gera cabeçalho do relatório.\"\"\"\n",
    "        video_name = Path(result.video_path).name\n",
    "        return f\"\"\"# Relatório de Análise de Vídeo\n",
    "## Tech Challenge - Fase 4\n",
    "\n",
    "**Arquivo**: `{video_name}`  \n",
    "**Duração**: {result.duration_seconds:.1f} segundos ({result.total_frames} frames @ {result.fps:.1f} FPS)  \n",
    "**Data da Análise**: {datetime.now().strftime('%d/%m/%Y %H:%M')}  \n",
    "**Tempo de Processamento**: {result.processing_time_seconds:.1f} segundos\n",
    "\"\"\"\n",
    "    \n",
    "    def _generate_statistics(self, result: VideoAnalysisResult) -> str:\n",
    "        \"\"\"Gera seção de estatísticas.\"\"\"\n",
    "        return f\"\"\"## Estatísticas Gerais\n",
    "\n",
    "| Métrica | Valor |\n",
    "|---------|-------|\n",
    "| Total de Frames Analisados | {result.total_frames} |\n",
    "| Rostos Detectados (total) | {result.total_faces_detected} |\n",
    "| Pessoas Únicas Identificadas | {result.unique_faces} |\n",
    "| Anomalias Detectadas | {result.total_anomalies} |\n",
    "\"\"\"\n",
    "    \n",
    "    def _generate_emotions_section(self, result: VideoAnalysisResult) -> str:\n",
    "        \"\"\"Gera seção de análise de emoções.\"\"\"\n",
    "        if not result.emotions_summary:\n",
    "            return \"## Análise de Emoções\\n\\n*Nenhuma emoção detectada.*\"\n",
    "        \n",
    "        # Traduz e ordena emoções\n",
    "        emotions_translated = {}\n",
    "        for emotion, count in result.emotions_summary.items():\n",
    "            emotion_pt = EMOTION_LABELS.get(emotion, emotion)\n",
    "            emotions_translated[emotion_pt] = count\n",
    "        \n",
    "        sorted_emotions = sorted(\n",
    "            emotions_translated.items(), \n",
    "            key=lambda x: -x[1]\n",
    "        )\n",
    "        \n",
    "        table = \"| Emoção | Frequência |\\n|--------|------------|\\n\"\n",
    "        for emotion, count in sorted_emotions:\n",
    "            table += f\"| {emotion} | {count} |\\n\"\n",
    "        \n",
    "        # Emoção dominante\n",
    "        dominant = sorted_emotions[0][0] if sorted_emotions else \"N/A\"\n",
    "        \n",
    "        return f\"\"\"## Análise de Emoções\n",
    "\n",
    "**Emoção Predominante**: {dominant}\n",
    "\n",
    "{table}\n",
    "\"\"\"\n",
    "    \n",
    "    def _generate_activities_section(self, result: VideoAnalysisResult) -> str:\n",
    "        \"\"\"Gera seção de análise de atividades.\"\"\"\n",
    "        if not result.activities_summary:\n",
    "            return \"## Detecção de Atividades\\n\\n*Nenhuma atividade detectada.*\"\n",
    "        \n",
    "        sorted_activities = sorted(\n",
    "            result.activities_summary.items(),\n",
    "            key=lambda x: -x[1]\n",
    "        )\n",
    "        \n",
    "        table = \"| Atividade | Frequência |\\n|-----------|------------|\\n\"\n",
    "        for activity, count in sorted_activities:\n",
    "            table += f\"| {activity} | {count} |\\n\"\n",
    "        \n",
    "        return f\"\"\"## Detecção de Atividades\n",
    "\n",
    "{table}\n",
    "\"\"\"\n",
    "    \n",
    "    def _generate_anomalies_section(self, result: VideoAnalysisResult) -> str:\n",
    "        \"\"\"Gera seção de anomalias.\"\"\"\n",
    "        if result.total_anomalies == 0:\n",
    "            return \"## Anomalias Detectadas\\n\\n*Nenhuma anomalia detectada durante a análise.*\"\n",
    "        \n",
    "        # Tabela por tipo\n",
    "        type_table = \"| Tipo de Anomalia | Quantidade |\\n|------------------|------------|\\n\"\n",
    "        for atype, count in result.anomalies_by_type.items():\n",
    "            type_table += f\"| {atype} | {count} |\\n\"\n",
    "        \n",
    "        # Lista detalhada (limitada a 20 eventos)\n",
    "        events_list = \"\"\n",
    "        for i, event in enumerate(result.anomaly_events[:20], 1):\n",
    "            events_list += f\"\\n### {i}. {event['tipo'].replace('_', ' ').title()}\\n\"\n",
    "            events_list += f\"- **Timestamp**: {event['timestamp']}\\n\"\n",
    "            events_list += f\"- **Frame**: {event['frame']}\\n\"\n",
    "            events_list += f\"- **Severidade**: {event['severidade']}\\n\"\n",
    "            events_list += f\"- **Descrição**: {event['descricao']}\\n\"\n",
    "        \n",
    "        more_events = \"\"\n",
    "        if len(result.anomaly_events) > 20:\n",
    "            more_events = f\"\\n*... e mais {len(result.anomaly_events) - 20} eventos.*\"\n",
    "        \n",
    "        return f\"\"\"## Anomalias Detectadas\n",
    "\n",
    "**Total**: {result.total_anomalies} anomalias\n",
    "\n",
    "### Distribuição por Tipo\n",
    "\n",
    "{type_table}\n",
    "\n",
    "### Detalhamento dos Eventos\n",
    "{events_list}{more_events}\n",
    "\"\"\"\n",
    "    \n",
    "    def _generate_llm_summary(self, result: VideoAnalysisResult) -> str:\n",
    "        \"\"\"Gera resumo usando LLM.\"\"\"\n",
    "        if not self.llm:\n",
    "            return self._generate_template_summary(result)\n",
    "        \n",
    "        # Prepara dados para o prompt\n",
    "        data = {\n",
    "            \"duracao\": f\"{result.duration_seconds:.1f}s\",\n",
    "            \"frames\": result.total_frames,\n",
    "            \"pessoas\": result.unique_faces,\n",
    "            \"emocoes\": result.emotions_summary,\n",
    "            \"atividades\": result.activities_summary,\n",
    "            \"anomalias\": result.total_anomalies,\n",
    "            \"tipos_anomalia\": result.anomalies_by_type\n",
    "        }\n",
    "        \n",
    "        prompt = self.PromptTemplate.from_template(\"\"\"\n",
    "Você é um analista de segurança e comportamento. Com base nos dados de análise de vídeo abaixo,\n",
    "escreva um resumo executivo em português brasileiro de 3-4 parágrafos.\n",
    "\n",
    "Dados da análise:\n",
    "- Duração do vídeo: {duracao}\n",
    "- Total de frames: {frames}\n",
    "- Pessoas identificadas: {pessoas}\n",
    "- Emoções detectadas: {emocoes}\n",
    "- Atividades detectadas: {atividades}\n",
    "- Número de anomalias: {anomalias}\n",
    "- Tipos de anomalia: {tipos_anomalia}\n",
    "\n",
    "O resumo deve:\n",
    "1. Descrever o cenário geral observado no vídeo\n",
    "2. Destacar as emoções e atividades predominantes\n",
    "3. Comentar sobre as anomalias detectadas (se houver)\n",
    "4. Fornecer insights relevantes sobre o comportamento observado\n",
    "\n",
    "Resumo:\n",
    "\"\"\")\n",
    "        \n",
    "        try:\n",
    "            chain = prompt | self.llm\n",
    "            response = chain.invoke(data)\n",
    "            return response.content.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"[ERRO] LLM: {e}\")\n",
    "            return self._generate_template_summary(result)\n",
    "    \n",
    "    def _generate_template_summary(self, result: VideoAnalysisResult) -> str:\n",
    "        \"\"\"Gera resumo usando template (sem LLM).\"\"\"\n",
    "        # Emoção dominante\n",
    "        dominant_emotion = \"N/A\"\n",
    "        if result.emotions_summary:\n",
    "            dominant_emotion = EMOTION_LABELS.get(\n",
    "                max(result.emotions_summary, key=result.emotions_summary.get),\n",
    "                \"Desconhecida\"\n",
    "            )\n",
    "        \n",
    "        # Atividade dominante\n",
    "        dominant_activity = \"N/A\"\n",
    "        if result.activities_summary:\n",
    "            dominant_activity = max(\n",
    "                result.activities_summary, \n",
    "                key=result.activities_summary.get\n",
    "            )\n",
    "        \n",
    "        # Monta resumo\n",
    "        summary = f\"\"\"O vídeo analisado tem duração de {result.duration_seconds:.1f} segundos e contém {result.total_frames} frames.\n",
    "\n",
    "Durante a análise, foram identificadas **{result.unique_faces} pessoa(s)** com um total de **{result.total_faces_detected} detecções de rostos**. A emoção predominante observada foi **{dominant_emotion}**, enquanto a atividade mais frequente foi **{dominant_activity}**.\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        if result.total_anomalies > 0:\n",
    "            summary += f\"\"\"Foram detectadas **{result.total_anomalies} anomalias** comportamentais durante o vídeo. \"\"\"\n",
    "            \n",
    "            if result.anomalies_by_type:\n",
    "                types_desc = \", \".join([\n",
    "                    f\"{count} {atype.replace('_', ' ')}\"\n",
    "                    for atype, count in result.anomalies_by_type.items()\n",
    "                ])\n",
    "                summary += f\"Os tipos identificados incluem: {types_desc}.\"\n",
    "        else:\n",
    "            summary += \"Não foram detectadas anomalias significativas durante o período analisado.\"\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def save_json_report(\n",
    "        self,\n",
    "        analysis_result: VideoAnalysisResult,\n",
    "        output_path: str\n",
    "    ):\n",
    "        \"\"\"Salva relatório em formato JSON.\"\"\"\n",
    "        data = {\n",
    "            \"video_path\": analysis_result.video_path,\n",
    "            \"total_frames\": analysis_result.total_frames,\n",
    "            \"fps\": analysis_result.fps,\n",
    "            \"duration_seconds\": analysis_result.duration_seconds,\n",
    "            \"total_faces_detected\": analysis_result.total_faces_detected,\n",
    "            \"unique_faces\": analysis_result.unique_faces,\n",
    "            \"emotions_summary\": analysis_result.emotions_summary,\n",
    "            \"activities_summary\": analysis_result.activities_summary,\n",
    "            \"total_anomalies\": analysis_result.total_anomalies,\n",
    "            \"anomalies_by_type\": analysis_result.anomalies_by_type,\n",
    "            \"anomaly_events\": analysis_result.anomaly_events,\n",
    "            \"processing_time_seconds\": analysis_result.processing_time_seconds,\n",
    "            \"generated_at\": datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        Path(output_path).write_text(\n",
    "            json.dumps(data, indent=2, ensure_ascii=False),\n",
    "            encoding='utf-8'\n",
    "        )\n",
    "        print(f\"[INFO] Relatório JSON salvo em: {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /content/TC-4/src/visualizer.py\n",
    "\"\"\"\n",
    "Tech Challenge - Fase 4: Módulo de Visualização\n",
    "Funções para desenhar detecções e anotações em frames.\n",
    "\"\"\"\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Optional\n",
    "from PIL import Image as PILImage, ImageDraw, ImageFont\n",
    "\n",
    "from .face_detector import FaceDetection\n",
    "from .emotion_analyzer import EmotionResult\n",
    "from .activity_detector import ActivityDetection\n",
    "from .anomaly_detector import AnomalyEvent\n",
    "\n",
    "\n",
    "# Cores padrão (RGB para PIL)\n",
    "COLORS = {\n",
    "    \"face\": (0, 255, 0),       # Verde\n",
    "    \"emotion\": (0, 255, 255),  # Ciano\n",
    "    \"activity\": (255, 165, 0), # Laranja\n",
    "    \"anomaly\": (255, 0, 0),    # Vermelho\n",
    "    \"text\": (255, 255, 255),   # Branco\n",
    "}\n",
    "\n",
    "\n",
    "def _get_font(size: int = 20) -> ImageFont.FreeTypeFont:\n",
    "    \"\"\"Obtém fonte com suporte a UTF-8.\"\"\"\n",
    "    font_paths = [\n",
    "        \"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\",\n",
    "        \"/usr/share/fonts/TTF/DejaVuSans.ttf\",\n",
    "        \"/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf\",\n",
    "    ]\n",
    "    for path in font_paths:\n",
    "        try:\n",
    "            return ImageFont.truetype(path, size)\n",
    "        except (IOError, OSError):\n",
    "            continue\n",
    "    return ImageFont.load_default()\n",
    "\n",
    "\n",
    "def put_text(\n",
    "    img: np.ndarray, \n",
    "    text: str, \n",
    "    position: Tuple[int, int], \n",
    "    font_size: int = 20, \n",
    "    color: Tuple[int, int, int] = (255, 255, 255)\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Adiciona texto com suporte a UTF-8 usando PIL.\"\"\"\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    pil_img = PILImage.fromarray(img_rgb)\n",
    "    draw = ImageDraw.Draw(pil_img)\n",
    "    font = _get_font(font_size)\n",
    "    \n",
    "    x, y = position\n",
    "    # Borda preta para contraste\n",
    "    for dx, dy in [(-1, -1), (-1, 1), (1, -1), (1, 1)]:\n",
    "        draw.text((x + dx, y + dy), text, font=font, fill=(0, 0, 0))\n",
    "    draw.text(position, text, font=font, fill=color)\n",
    "    \n",
    "    return cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "\n",
    "def draw_detections(\n",
    "    frame: np.ndarray,\n",
    "    faces: List[FaceDetection],\n",
    "    emotions: List[Optional[EmotionResult]],\n",
    "    activities: List[ActivityDetection],\n",
    "    anomalies: List[AnomalyEvent],\n",
    "    min_face_size: int = 40,\n",
    "    min_emotion_conf: float = 0.3,\n",
    "    min_activity_conf: float = 0.4\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Desenha todas as detecções no frame.\n",
    "    \n",
    "    Args:\n",
    "        frame: Frame BGR\n",
    "        faces: Lista de detecções de faces\n",
    "        emotions: Lista de resultados de emoções (pode ter None)\n",
    "        activities: Lista de detecções de atividades\n",
    "        anomalies: Lista de anomalias detectadas\n",
    "        min_face_size: Tamanho mínimo de face para exibir\n",
    "        min_emotion_conf: Confiança mínima para exibir emoção\n",
    "        min_activity_conf: Confiança mínima para exibir atividade\n",
    "    \n",
    "    Returns:\n",
    "        Frame anotado\n",
    "    \"\"\"\n",
    "    annotated = frame.copy()\n",
    "    h, w = frame.shape[:2]\n",
    "    \n",
    "    # Filtra faces válidas\n",
    "    valid_faces = [f for f in faces if _is_valid_face(f, w, h, min_face_size)]\n",
    "    \n",
    "    # Desenha faces\n",
    "    for i, face in enumerate(valid_faces):\n",
    "        x, y, fw, fh = face.bbox\n",
    "        cv2.rectangle(annotated, (x, y), (x + fw, y + fh), (0, 255, 0), 2)\n",
    "        annotated = put_text(annotated, f\"ID:{face.face_id}\", (x, max(0, y - 25)), 18, COLORS[\"face\"])\n",
    "        \n",
    "        # Emoção correspondente\n",
    "        if i < len(emotions) and emotions[i] is not None:\n",
    "            emotion = emotions[i]\n",
    "            if emotion.confidence >= min_emotion_conf:\n",
    "                text = f\"{emotion.emotion_pt}: {emotion.confidence:.0%}\"\n",
    "                annotated = put_text(annotated, text, (x, y + fh + 5), 16, COLORS[\"emotion\"])\n",
    "    \n",
    "    # Desenha atividades (apenas de pessoas detectadas pelo YOLO)\n",
    "    for activity in activities:\n",
    "        if activity.confidence < min_activity_conf:\n",
    "            continue\n",
    "        if activity.bbox:\n",
    "            ax, ay, aw, ah = activity.bbox\n",
    "            # Desenha bbox da pessoa (azul)\n",
    "            cv2.rectangle(annotated, (ax, ay), (ax + aw, ay + ah), (255, 100, 0), 1)\n",
    "            annotated = put_text(annotated, activity.activity_pt, (ax, max(0, ay - 10)), 18, COLORS[\"activity\"])\n",
    "    \n",
    "    # Indicador de anomalias\n",
    "    if anomalies:\n",
    "        annotated = put_text(annotated, f\"⚠ {len(anomalies)} ANOMALIA(S)\", (10, 10), 24, COLORS[\"anomaly\"])\n",
    "    \n",
    "    return annotated\n",
    "\n",
    "\n",
    "def _is_valid_face(\n",
    "    face: FaceDetection, \n",
    "    frame_w: int, \n",
    "    frame_h: int, \n",
    "    min_size: int = 40\n",
    ") -> bool:\n",
    "    \"\"\"Valida se uma detecção de face é plausível.\"\"\"\n",
    "    x, y, w, h = face.bbox\n",
    "    \n",
    "    # Tamanho mínimo\n",
    "    if w < min_size or h < min_size:\n",
    "        return False\n",
    "    \n",
    "    # Proporção válida (rostos são aproximadamente quadrados)\n",
    "    aspect = w / h if h > 0 else 0\n",
    "    if aspect < 0.5 or aspect > 2.0:\n",
    "        return False\n",
    "    \n",
    "    # Dentro dos limites\n",
    "    if x < 0 or y < 0 or x + w > frame_w or y + h > frame_h:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "def show_frame(frame: np.ndarray, title: str = \"Frame\", figsize: Tuple[int, int] = (12, 8)):\n",
    "    \"\"\"Exibe um frame no notebook usando matplotlib.\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=figsize)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(rgb_frame)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /content/TC-4/src/__init__.py\n",
    "# Tech Challenge - Fase 4\n",
    "# Análise de Vídeo com Reconhecimento Facial, Emoções e Atividades\n",
    "from .config import VIDEO_PATH, OUTPUT_DIR, REPORTS_DIR, INPUT_DIR\n",
    "from .face_detector import FaceDetector, FaceDetection\n",
    "from .emotion_analyzer import EmotionAnalyzer, EmotionResult\n",
    "from .activity_detector import ActivityDetector, ActivityDetection, ActivityType\n",
    "from .anomaly_detector import AnomalyDetector, AnomalyEvent\n",
    "from .report_generator import ReportGenerator\n",
    "from .visualizer import draw_detections, put_text, show_frame\n",
    "\n",
    "__all__ = [\n",
    "    \"VIDEO_PATH\", \"OUTPUT_DIR\", \"REPORTS_DIR\", \"INPUT_DIR\",\n",
    "    \"FaceDetector\", \"FaceDetection\",\n",
    "    \"EmotionAnalyzer\", \"EmotionResult\", \n",
    "    \"ActivityDetector\", \"ActivityDetection\", \"ActivityType\",\n",
    "    \"AnomalyDetector\", \"AnomalyEvent\",\n",
    "    \"ReportGenerator\",\n",
    "    \"draw_detections\", \"put_text\", \"show_frame\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recarrega módulos para garantir que as alterações tenham efeito\n",
    "import importlib\n",
    "try:\n",
    "    import src.config\n",
    "    importlib.reload(src.config)\n",
    "except ImportError:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, deque\n",
    "from IPython.display import display, HTML, Video\n",
    "import base64\n",
    "\n",
    "# Importa módulos do projeto\n",
    "from src import (\n",
    "    OUTPUT_DIR, REPORTS_DIR,\n",
    "    FaceDetector, EmotionAnalyzer, ActivityDetector, \n",
    "    AnomalyDetector, ReportGenerator,\n",
    "    draw_detections, show_frame\n",
    ")\n",
    "from src.config import ACTIVITY_CATEGORIES, EMOTION_LABELS, FRAME_SKIP\n",
    "\n",
    "# Define vídeo (usa o encontrado ou padrão)\n",
    "VIDEO_PATH_USE = globals().get('VIDEO_PATH_FOUND')\n",
    "if not VIDEO_PATH_USE:\n",
    "    print(\"ERRO: Nenhum vídeo definido. Por favor, faça upload de um vídeo em /content/TC-4/input/\")\n",
    "else:\n",
    "    print(f\"Iniciando análise de: {VIDEO_PATH_USE}\")\n",
    "\n",
    "# Configurações locais\n",
    "CONFIG = {\n",
    "    \"detection_interval\": FRAME_SKIP,\n",
    "    \"min_face_size\": 40,\n",
    "    \"output_video\": str(OUTPUT_DIR / \"video_analisado.mp4\")\n",
    "}\n",
    "\n",
    "# Inicializa Detectores\n",
    "print(\"Carregando modelos IA...\")\n",
    "face_detector = FaceDetector(method=\"mediapipe\") # MediaPipe é leve e bom para Colab\n",
    "emotion_analyzer = EmotionAnalyzer(method=\"fer\") # FER funciona bem no Colab com GPU/CPU\n",
    "activity_detector = ActivityDetector(model_size=\"s\") # Usando Small para balancear\n",
    "anomaly_detector = AnomalyDetector()\n",
    "\n",
    "print(\"Modelos carregados!\")\n",
    "\n",
    "if VIDEO_PATH_USE:\n",
    "    # Abre vídeo\n",
    "    cap = cv2.VideoCapture(VIDEO_PATH_USE)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    print(f\"Vídeo: {width}x{height} @ {fps:.1f} fps ({total_frames} frames)\")\n",
    "\n",
    "    # Configura Gravador de Vídeo\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(CONFIG[\"output_video\"], fourcc, fps, (width, height))\n",
    "\n",
    "    # Estatísticas\n",
    "    stats = {\n",
    "        \"faces\": 0,\n",
    "        \"emotions\": Counter(),\n",
    "        \"activities\": Counter(),\n",
    "        \"anomalies\": Counter()\n",
    "    }\n",
    "\n",
    "    # Cache para frames intermediários\n",
    "    cache = {\"faces\": [], \"emotions\": [], \"activities\": [], \"anomalies\": []}\n",
    "\n",
    "    frame_idx = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Processando {total_frames} frames...\")\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Detecção completa a cada N frames\n",
    "        if frame_idx % CONFIG[\"detection_interval\"] == 0:\n",
    "            # Faces\n",
    "            cache[\"faces\"] = face_detector.detect(frame)\n",
    "            stats[\"faces\"] += len(cache[\"faces\"])\n",
    "            \n",
    "            # Emoções\n",
    "            cache[\"emotions\"] = []\n",
    "            for face in cache[\"faces\"]:\n",
    "                if face.bbox[2] >= CONFIG[\"min_face_size\"]:\n",
    "                    em = emotion_analyzer.analyze(frame, face.bbox, face.face_id)\n",
    "                    cache[\"emotions\"].append(em)\n",
    "                    if em:\n",
    "                        stats[\"emotions\"][em.emotion_pt] += 1\n",
    "                else:\n",
    "                    cache[\"emotions\"].append(None)\n",
    "            \n",
    "            # Atividades (YOLO)\n",
    "            cache[\"activities\"] = activity_detector.detect(frame)\n",
    "            for act in cache[\"activities\"]:\n",
    "                stats[\"activities\"][act.activity_pt] += 1\n",
    "            \n",
    "            # Anomalias\n",
    "            cache[\"anomalies\"] = anomaly_detector.update(\n",
    "                frame_idx, cache[\"faces\"], \n",
    "                [e for e in cache[\"emotions\"] if e],\n",
    "                cache[\"activities\"]\n",
    "            )\n",
    "            for a in cache[\"anomalies\"]:\n",
    "                stats[\"anomalies\"][a.anomaly_type] += 1\n",
    "        \n",
    "        # Desenha e salva\n",
    "        annotated = draw_detections(\n",
    "            frame, cache[\"faces\"], cache[\"emotions\"],\n",
    "            cache[\"activities\"], cache[\"anomalies\"],\n",
    "            CONFIG[\"min_face_size\"]\n",
    "        )\n",
    "        out.write(annotated)\n",
    "        \n",
    "        # Progresso a cada 10%\n",
    "        if frame_idx % (total_frames // 10) == 0 and frame_idx > 0:\n",
    "            pct = frame_idx / total_frames * 100\n",
    "            elapsed = time.time() - start_time\n",
    "            fps_proc = frame_idx / elapsed\n",
    "            print(f\"  {pct:.0f}% - FPS: {fps_proc:.1f}\")\n",
    "        \n",
    "        frame_idx += 1\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\nConcluído em {elapsed:.1f}s ({frame_idx/elapsed:.1f} fps)\")\n",
    "    print(f\"Vídeo salvo: {CONFIG['output_video']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibe Estatísticas\n",
    "print(\"=\" * 50)\n",
    "print(\"ESTATÍSTICAS DA ANÁLISE\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nFaces detectadas (total accum): {stats['faces']}\")\n",
    "print(f\"\\nEmoções (top 5):\")\n",
    "for em, count in stats[\"emotions\"].most_common(5):\n",
    "    print(f\"  - {em}: {count}\")\n",
    "print(f\"\\nAtividades (top 5):\")\n",
    "for act, count in stats[\"activities\"].most_common(5):\n",
    "    print(f\"  - {act}: {count}\")\n",
    "print(f\"\\nAnomalias: {sum(stats['anomalies'].values())}\")\n",
    "for anom, count in stats[\"anomalies\"].most_common():\n",
    "    print(f\"  - {anom}: {count}\")\n",
    "\n",
    "# Gráficos\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "if stats[\"emotions\"]:\n",
    "    em_data = stats[\"emotions\"].most_common(6)\n",
    "    axes[0].barh([x[0] for x in em_data], [x[1] for x in em_data], color='steelblue')\n",
    "    axes[0].set_title('Emoções Detectadas')\n",
    "\n",
    "if stats[\"activities\"]:\n",
    "    act_data = stats[\"activities\"].most_common(6)\n",
    "    axes[1].barh([x[0] for x in act_data], [x[1] for x in act_data], color='coral')\n",
    "    axes[1].set_title('Atividades Detectadas')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_video_colab(video_path):\n",
    "    mp4 = open(video_path, 'rb').read()\n",
    "    data_url = \"data:video/mp4;base64,\" + base64.b64encode(mp4).decode()\n",
    "    return HTML(f\"\"\"\n",
    "    <video width=800 controls>\n",
    "          <source src=\"{data_url}\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    \"\"\")\n",
    "\n",
    "import os\n",
    "if os.path.exists(CONFIG[\"output_video\"]):\n",
    "    file_size_mb = os.path.getsize(CONFIG[\"output_video\"]) / (1024 * 1024)\n",
    "    print(f\"Vídeo processado: {file_size_mb:.1f} MB\")\n",
    "    \n",
    "    # Se o vídeo for muito grande, o Colab pode travar ao tentar incorporar em base64\n",
    "    if file_size_mb > 50:\n",
    "        print(\"Vídeo muito grande para exibição inline (>50MB). Baixe o arquivo para visualizar.\")\n",
    "    else:\n",
    "        # Tenta exibir apenas se for pequeno\n",
    "        try:\n",
    "             display(show_video_colab(CONFIG[\"output_video\"]))\n",
    "        except Exception as e:\n",
    "             print(f\"Erro ao exibir vídeo: {e}\")\n",
    "else:\n",
    "    print(\"Arquivo de vídeo não encontrado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "if os.path.exists(CONFIG[\"output_video\"]):\n",
    "    print(\"Iniciando download...\")\n",
    "    files.download(CONFIG[\"output_video\"])\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
