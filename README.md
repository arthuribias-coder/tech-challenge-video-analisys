# Tech Challenge - Fase 4: An√°lise de V√≠deo com IA

Este projeto consiste em uma aplica√ß√£o desktop avan√ßada para an√°lise inteligente de v√≠deos, desenvolvida como parte do Tech Challenge (Fase 4). A solu√ß√£o utiliza t√©cnicas modernas de Vis√£o Computacional e Intelig√™ncia Artificial para extrair insights comportamentais, contextuais e emocionais de arquivos de v√≠deo.

## üéØ Objetivo

O objetivo principal √© processar v√≠deos de vigil√¢ncia ou monitoramento para identificar e rastrear pessoas, analisar suas atividades e emo√ß√µes, entender o contexto do ambiente (cena) e detectar anomalias (comportamentos suspeitos ou objetos fora de contexto).

## üöÄ Funcionalidades Principais

* **Detec√ß√£o e Rastreamento de Atividades**: Utiliza **YOLO11-pose** para identificar esqueletos e classificar a√ß√µes (caminhando, correndo, sentado, acenando, etc.).
* **An√°lise de Emo√ß√µes**: Integra√ß√£o com **DeepFace** para an√°lise facial profunda, identificando emo√ß√µes como alegria, tristeza, raiva, surpresa, etc.
* **Detec√ß√£o de Pessoas Deitadas (Oriented Object Detection)**: Uso inovador do **YOLO11-obb** (Oriented Bounding Box) para distinguir com precis√£o entre pessoas em p√© e deitadas, crucial para detec√ß√£o de quedas ou acidentes.
* **Classifica√ß√£o de Cena (Context Awareness)**: O sistema utiliza **YOLO11-cls** para identificar o ambiente (ex: escrit√≥rio, sala de estar, parque), permitindo valida√ß√µes contextuais.
* **Detec√ß√£o de Objetos Contextual**: Identifica objetos na cena (**YOLO11**) e valida se s√£o esperados ou an√¥malos para aquele ambiente (ex: uma cama em um escrit√≥rio √© uma anomalia).
* **Detec√ß√£o de Anomalias**: Motor de regras que combina dados comportamentais e visuais para alertar sobre:
  * Movimentos bruscos.
  * Picos de emo√ß√£o negativa.
  * Inatividade prolongada.
  * Inconsist√™ncias de cena (objetos proibidos).
  * Pessoas deitadas em locais inapropriados.
* **Interface Gr√°fica Profissional (GUI)**: Desenvolvida em **PyQt6**, com:
  * Player de v√≠deo integrado com controles
  * Preview em tempo real durante processamento
  * Gr√°ficos estat√≠sticos (emo√ß√µes, atividades, anomalias, objetos)
  * Painel de estat√≠sticas ao vivo
  * Configura√ß√µes avan√ßadas edit√°veis via JSON
  * Modo debug para an√°lise detalhada
* **Interface de Linha de Comando (CLI)**: Processamento em lote sem interface gr√°fica
* **Relat√≥rios Autom√°ticos**: Gera√ß√£o de relat√≥rios em TXT com resumo completo das ocorr√™ncias

## ‚öôÔ∏è Modos de Uso

### 1. Interface Gr√°fica (GUI) - Recomendado

A GUI oferece controle completo sobre o processamento com visualiza√ß√£o em tempo real:

```bash
python gui_app.py
```

**Recursos da GUI:**

* Sele√ß√£o de v√≠deo via di√°logo
* Configura√ß√£o de processamento (frame skip, FPS, GPU, modelos)
* Preview em tempo real (opcional, configur√°vel)
* Visualiza√ß√£o de estat√≠sticas durante processamento
* Gr√°ficos interativos por categoria
* Player de v√≠deo com controles de reprodu√ß√£o
* Modo debug com checkbox (ativa logs detalhados no console)
* Exporta√ß√£o de v√≠deo processado e relat√≥rio

### 2. Interface de Linha de Comando (CLI)

Para processamento automatizado ou em servidores sem interface gr√°fica:

```bash
# Uso b√°sico
python cli.py input/video.mp4

# Com debug ativado
python cli.py input/video.mp4 --debug

# For√ßando CPU (sem GPU)
python cli.py input/video.mp4 --no-gpu

# Especificando arquivo de sa√≠da
python cli.py input/video.mp4 --output output/resultado.mp4

# Com arquivo de configura√ß√£o customizado
python cli.py input/video.mp4 --config config/custom.json
```

**Par√¢metros CLI:**

* `video`: Caminho do arquivo de v√≠deo (obrigat√≥rio)
* `--config`: Arquivo JSON de configura√ß√£o customizada (opcional)
* `--debug`: Habilita logs detalhados no console
* `--output`: Caminho de sa√≠da para v√≠deo processado
* `--no-gpu`: For√ßa uso de CPU ao inv√©s de GPU

## ‚öôÔ∏è Configura√ß√£o e Ajustes

### Configura√ß√µes via GUI

1. Clique no bot√£o **"Configura√ß√µes"** (√≠cone de engrenagem) na toolbar
2. Ajuste os par√¢metros b√°sicos:
   * **Frame Skip**: Processa 1 a cada N frames (‚Üë = mais r√°pido, ‚Üì qualidade)
   * **FPS de Sa√≠da**: Taxa de frames do v√≠deo processado (15, 24, 30, 60)
   * **Preview**: Habilita visualiza√ß√£o em tempo real durante processamento
   * **FPS do Preview**: Controla quantos frames/segundo aparecem no preview (5-30)
   * **GPU/CPU**: Escolha o dispositivo de processamento
   * **Tamanho do Modelo**: nano (n), small (s), medium (m), large (l)
   * **Detec√ß√£o de Objetos**: Habilita/desabilita an√°lise de objetos

3. Para configura√ß√µes avan√ßadas, clique em **"Avan√ßado..."**:
   * Edite limiares de emo√ß√µes (sensibilidade por emo√ß√£o)
   * Ajuste par√¢metros de poses (√¢ngulos, dist√¢ncias)
   * Configure pesos contextuais de emo√ß√£o por tipo de cena
   * As altera√ß√µes s√£o salvas em `config/settings.json`

### Configura√ß√µes via Arquivo JSON

Edite diretamente `config/settings.json`:

```json
{
  "frame_skip": 2,
  "target_fps": 30,
  "enable_preview": true,
  "preview_fps": 10,
  "use_gpu": true,
  "model_size": "n",
  "enable_object_detection": true,
  "EMOTION_THRESHOLDS": {
    "neutral": 0.25,
    "sad": 0.60,
    "happy": 0.35,
    "surprise": 0.50,
    "fear": 0.70,
    "angry": 0.50,
    "disgust": 0.55
  }
}
```

### Modo Debug

Ative o checkbox **"Debug"** na toolbar para:

* Ver logs detalhados no console/terminal
* Acompanhar decis√µes dos detectores em tempo real
* Identificar problemas de detec√ß√£o
* Analisar performance frame a frame

### Requisitos de Hardware (GPU)

Para performance em tempo real, **recomenda-se fortemente o uso de GPU NVIDIA (CUDA)**.

* O sistema detecta automaticamente se `cuda` est√° dispon√≠vel.
* Voc√™ pode for√ßar CPU ou GPU nas configura√ß√µes da interface ou via `--no-gpu` no CLI.

**Instala√ß√£o PyTorch com CUDA:**

```bash
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121
```

## üèóÔ∏è Arquitetura e Fluxo de Processamento

A aplica√ß√£o segue uma arquitetura modular, onde uma Thread de Processamento (`ProcessorThreadQt`) orquestra a execu√ß√£o sequencial dos modelos de IA frame a frame, sem congelar a interface do usu√°rio.

### Pipeline de Processamento

O fluxo de an√°lise √© executado sequencialmente para cada frame processado:

```mermaid
flowchart TD
    Input[üìπ V√≠deo] --> Capture[üé¨ Captura]
    Capture --> Scene[üèûÔ∏è Cena]
    Scene --> Pose[üßç Poses]
    Pose --> OBB[‚Ü™Ô∏è Orienta√ß√£o]
    OBB --> Face[üë§ Faces]
    Face --> Emotion[üòä Emo√ß√µes]
    Face --> Object[üì¶ Objetos]
    Object --> Anomaly[‚ö†Ô∏è Anomalias]
    Anomaly --> Gui[üé® Interface]
```

| Ordem | M√≥dulo | Fun√ß√£o Principal | Tecnologia |
| :---: | :--- | :--- | :--- |
| **1** | **SceneClassifier** | Identifica o contexto do ambiente (ex: "Escrit√≥rio", "Parque") | YOLO11-cls |
| **2** | **OrientedDetector** | Detecta a orienta√ß√£o de pessoas (em p√© vs. deitado) | YOLO11-obb |
| **3** | **ActivityDetector** | Extrai poses esquel√©ticas e classifica a√ß√µes | YOLO11-pose |
| **4** | **FaceDetector** | Recorta rostos baseando-se na geometria do corpo | Heur√≠stica |
| **5** | **EmotionAnalyzer** | Analisa express√µes faciais nos recortes | DeepFace |
| **6** | **ObjectDetector** | Detecta objetos e valida coer√™ncia com a cena | YOLO11-detect |
| **7** | **AnomalyDetector** | Aplica regras para identificar comportamentos suspeitos | L√≥gica |
| **8** | **Visualizer** | Renderiza anota√ß√µes e atualiza os gr√°ficos | OpenCV/Qt |

### Estrutura do Projeto

```text
TC-4/
‚îú‚îÄ‚îÄ gui_app.py              # Ponto de entrada da aplica√ß√£o
‚îú‚îÄ‚îÄ requirements.txt        # Lista de depend√™ncias Python
‚îú‚îÄ‚îÄ input/                  # Diret√≥rio para v√≠deos de entrada
‚îú‚îÄ‚îÄ output/                 # Diret√≥rio para v√≠deos processados
‚îú‚îÄ‚îÄ reports/                # Relat√≥rios gerados
‚îú‚îÄ‚îÄ models/                 # Pesos dos modelos YOLO e DeepFace
‚îî‚îÄ‚îÄ src/
    ‚îú‚îÄ‚îÄ config.py           # Configura√ß√µes globais e regras de contexto
    ‚îú‚îÄ‚îÄ gui/                # Interface Gr√°fica (PyQt6)
    ‚îÇ   ‚îú‚îÄ‚îÄ main_window_qt.py
    ‚îÇ   ‚îî‚îÄ‚îÄ threads/processor_thread_qt.py # Orquestrador do pipeline
    ‚îú‚îÄ‚îÄ activity_detector.py # Wrapper YOLO11-pose
    ‚îú‚îÄ‚îÄ emotion_analyzer.py  # Wrapper DeepFace
    ‚îú‚îÄ‚îÄ face_detector.py     # L√≥gica de extra√ß√£o facial
    ‚îú‚îÄ‚îÄ scene_classifier.py  # Wrapper YOLO11-cls
    ‚îú‚îÄ‚îÄ oriented_detector.py # Wrapper YOLO11-obb
    ‚îú‚îÄ‚îÄ object_detector.py   # Wrapper YOLO11-detect
    ‚îú‚îÄ‚îÄ anomaly_detector.py  # Motor de regras de anomalia
    ‚îî‚îÄ‚îÄ visualizer.py        # Renderiza√ß√£o visual (OpenCV/PIL)
```

## üõ†Ô∏è Instala√ß√£o e Configura√ß√£o

### Pr√©-requisitos

* **Python 3.10** ou superior (3.12 recomendado).
* **GPU NVIDIA** (Opcional, mas altamente recomendado para performance em tempo real). Drivers CUDA instalados.

### Passo a Passo

1. **Clone o reposit√≥rio e navegue at√© a pasta:**

    ```bash
    git clone <url-do-repositorio>
    cd TC-4
    ```

2. **Crie e ative um ambiente virtual:**

    ```bash
    python3 -m venv .venv
    source .venv/bin/activate  # Linux/Mac
    # .venv\Scripts\activate   # Windows
    ```

3. **Instale as depend√™ncias:**

    ```bash
    pip install -r requirements.txt
    ```

    *Nota: A primeira execu√ß√£o baixar√° automaticamente os modelos YOLO (~100MB cada) e DeepFace.*

4. **Execute a aplica√ß√£o:**

    ```bash
    python gui_app.py
    ```

## üìñ Como Usar

1. A interface abrir√° automaticamente.
2. Clique no √≠cone de **"Abrir Arquivo"** (canto superior esquerdo) para selecionar um v√≠deo da pasta `input/`.
3. Ajuste as configura√ß√µes se necess√°rio (bot√£o "Configura√ß√µes"):
    * **Frame Skip**: Aumente para maior velocidade (ex: 2 ou 3).
    * **Device**: CPU ou CUDA (GPU).
4. Clique no bot√£o **Play** (‚ñ∂) para iniciar a an√°lise.
5. Acompanhe os resultados em tempo real:
    * **V√≠deo**: Visualiza√ß√£o com anota√ß√µes de bounding boxes e labels.
    * **Estat√≠sticas**: Contadores de faces, anomalias e atividades.
    * **Gr√°ficos**: Distribui√ß√£o de emo√ß√µes e atividades (abas na parte inferior).
6. Ao final, o v√≠deo processado ser√° salvo na pasta `output/` e um relat√≥rio de texto em `reports/`.

## üìä Relat√≥rio Autom√°tico

Ap√≥s o processamento, a aplica√ß√£o gera automaticamente um **relat√≥rio detalhado** em formato Markdown (`.md`) na pasta `reports/`. O relat√≥rio inclui:

### Conte√∫do do Relat√≥rio

1. **Resumo Executivo**: Vis√£o geral da an√°lise com principais insights
2. **Estat√≠sticas Gerais**:
   * ‚úÖ **Total de frames analisados**
   * ‚úÖ **N√∫mero de anomalias detectadas**
   * Rostos e pessoas identificadas
   * FPS e tempo de processamento
3. **An√°lise de Emo√ß√µes**: Distribui√ß√£o detalhada com gr√°ficos textuais e percentuais
4. **Detec√ß√£o de Atividades**: Frequ√™ncia de cada atividade com percentuais
5. **Anomalias Comportamentais**:
   * Distribui√ß√£o por tipo
   * Detalhamento de cada evento (timestamp, frame, severidade, descri√ß√£o)
6. **Metodologia e Tecnologias**: Modelos utilizados e crit√©rios de detec√ß√£o
7. **Observa√ß√£o Importante**:
   * ‚ö†Ô∏è **Movimento an√¥malo n√£o segue o padr√£o geral de atividades** (como gestos bruscos ou comportamentos at√≠picos)
   * Esses s√£o classificados como an√¥malos pela aplica√ß√£o

### Exemplo de Localiza√ß√£o

```plaintext
reports/
‚îú‚îÄ‚îÄ analise_video1_20260109_143022.md    # Relat√≥rio em Markdown
‚îî‚îÄ‚îÄ analise_video1_20260109_143022.json  # Dados estruturados (JSON)
```

The relat√≥rio pode ser visualizado diretamente no GitHub ou em qualquer visualizador Markdown.

## üé• Demonstra√ß√£o em V√≠deo

Para demonstra√ß√£o completa das funcionalidades implementadas, um v√≠deo de demonstra√ß√£o est√° dispon√≠vel evidenciando:

1. **Inicializa√ß√£o da Aplica√ß√£o**: Interface gr√°fica PyQt6
2. **Sele√ß√£o e Configura√ß√£o**: Escolha de v√≠deo e ajuste de par√¢metros
3. **Processamento em Tempo Real**:
   * Preview do v√≠deo sendo processado
   * Estat√≠sticas atualizadas em tempo real
   * Gr√°ficos de emo√ß√µes e atividades
4. **Detec√ß√£o de Atividades**: Pessoas andando, sentadas, acenando, etc.
5. **An√°lise de Emo√ß√µes**: Classifica√ß√£o facial em tempo real
6. **Detec√ß√£o de Anomalias**: Identifica√ß√£o de comportamentos at√≠picos
7. **Contexto de Cena**: Classifica√ß√£o autom√°tica do ambiente
8. **Relat√≥rio Final**: Visualiza√ß√£o do relat√≥rio gerado

**Link do V√≠deo**: [Adicionar link do v√≠deo de demonstra√ß√£o aqui]

## ‚öôÔ∏è Configura√ß√£o T√©cnica (`src/config.py`)

O arquivo `src/config.py` centraliza constantes importantes, como:

* `SCENE_CONTEXT_RULES`: Dicion√°rio que define quais objetos s√£o esperados ou an√¥malos em cada tipo de cena (escrit√≥rio, casa, rua).
* `ANOMALY_THRESHOLDS`: Limiares de sensibilidade para detec√ß√£o de anomalias.
* `EMOTION_THRESHOLDS`: Sensibilidade para cada tipo de emo√ß√£o.

---

## Tech Challenge Fase 4 - P√≥s Tech Data Analytics
