# Tech Challenge - Fase 4: AnÃ¡lise de VÃ­deo com IA

Este projeto consiste em uma aplicaÃ§Ã£o desktop avanÃ§ada para anÃ¡lise inteligente de vÃ­deos, desenvolvida como parte do Tech Challenge (Fase 4). A soluÃ§Ã£o utiliza tÃ©cnicas modernas de VisÃ£o Computacional e InteligÃªncia Artificial para extrair insights comportamentais, contextuais e emocionais de arquivos de vÃ­deo.

## ğŸ¯ Objetivo

O objetivo principal Ã© processar vÃ­deos de vigilÃ¢ncia ou monitoramento para identificar e rastrear pessoas, analisar suas atividades e emoÃ§Ãµes, entender o contexto do ambiente (cena) e detectar anomalias (comportamentos suspeitos ou objetos fora de contexto).

## ğŸš€ Funcionalidades Principais

* **DetecÃ§Ã£o e Rastreamento de Atividades**: Utiliza **YOLO11-pose** para identificar esqueletos e classificar aÃ§Ãµes (caminhando, correndo, sentado, acenando, etc.).
* **AnÃ¡lise de EmoÃ§Ãµes**: IntegraÃ§Ã£o com **DeepFace** para anÃ¡lise facial profunda, identificando emoÃ§Ãµes como alegria, tristeza, raiva, surpresa, etc.
* **DetecÃ§Ã£o de Pessoas Deitadas (Oriented Object Detection)**: Uso inovador do **YOLO11-obb** (Oriented Bounding Box) para distinguir com precisÃ£o entre pessoas em pÃ© e deitadas, crucial para detecÃ§Ã£o de quedas ou acidentes.
* **ClassificaÃ§Ã£o de Cena (Context Awareness)**: O sistema utiliza **YOLO11-cls** para identificar o ambiente (ex: escritÃ³rio, sala de estar, parque), permitindo validaÃ§Ãµes contextuais.
* **DetecÃ§Ã£o de Objetos Contextual**: Identifica objetos na cena (**YOLO11**) e valida se sÃ£o esperados ou anÃ´malos para aquele ambiente (ex: uma cama em um escritÃ³rio Ã© uma anomalia).
* **DetecÃ§Ã£o de Anomalias**: Motor de regras que combina dados comportamentais e visuais para alertar sobre:
  * Movimentos bruscos.
  * Picos de emoÃ§Ã£o negativa.
  * Inatividade prolongada.
  * InconsistÃªncias de cena (objetos proibidos).
  * Pessoas deitadas em locais inapropriados.
* **Interface GrÃ¡fica Profissional (GUI)**: Desenvolvida em **PyQt6**, com:
  * Player de vÃ­deo integrado com controles
  * Preview em tempo real durante processamento
  * GrÃ¡ficos estatÃ­sticos (emoÃ§Ãµes, atividades, anomalias, objetos)
  * Painel de estatÃ­sticas ao vivo
  * ConfiguraÃ§Ãµes avanÃ§adas editÃ¡veis via JSON
  * Modo debug para anÃ¡lise detalhada
* **Interface de Linha de Comando (CLI)**: Processamento em lote sem interface grÃ¡fica
* **RelatÃ³rios AutomÃ¡ticos**: GeraÃ§Ã£o de relatÃ³rios em TXT com resumo completo das ocorrÃªncias

## âš™ï¸ Modos de Uso

### 1. Interface GrÃ¡fica (GUI) - Recomendado

A GUI oferece controle completo sobre o processamento com visualizaÃ§Ã£o em tempo real:

```bash
python gui_app.py
```

**Recursos da GUI:**

* SeleÃ§Ã£o de vÃ­deo via diÃ¡logo
* ConfiguraÃ§Ã£o de processamento (frame skip, FPS, GPU, modelos)
* Preview em tempo real (opcional, configurÃ¡vel)
* VisualizaÃ§Ã£o de estatÃ­sticas durante processamento
* GrÃ¡ficos interativos por categoria
* Player de vÃ­deo com controles de reproduÃ§Ã£o
* Modo debug com checkbox (ativa logs detalhados no console)
* ExportaÃ§Ã£o de vÃ­deo processado e relatÃ³rio

### 2. Interface de Linha de Comando (CLI)

Para processamento automatizado ou em servidores sem interface grÃ¡fica:

```bash
# Uso bÃ¡sico
python cli.py input/video.mp4

# Com debug ativado
python cli.py input/video.mp4 --debug

# ForÃ§ando CPU (sem GPU)
python cli.py input/video.mp4 --no-gpu

# Especificando arquivo de saÃ­da
python cli.py input/video.mp4 --output output/resultado.mp4

# Com arquivo de configuraÃ§Ã£o customizado
python cli.py input/video.mp4 --config config/custom.json
```

**ParÃ¢metros CLI:**

* `video`: Caminho do arquivo de vÃ­deo (obrigatÃ³rio)
* `--config`: Arquivo JSON de configuraÃ§Ã£o customizada (opcional)
* `--debug`: Habilita logs detalhados no console
* `--output`: Caminho de saÃ­da para vÃ­deo processado
* `--no-gpu`: ForÃ§a uso de CPU ao invÃ©s de GPU

## âš™ï¸ ConfiguraÃ§Ã£o e Ajustes

### ConfiguraÃ§Ãµes via GUI

1. Clique no botÃ£o **"ConfiguraÃ§Ãµes"** (Ã­cone de engrenagem) na toolbar
2. Ajuste os parÃ¢metros bÃ¡sicos:
   * **Frame Skip**: Processa 1 a cada N frames (â†‘ = mais rÃ¡pido, â†“ qualidade)
   * **FPS de SaÃ­da**: Taxa de frames do vÃ­deo processado (15, 24, 30, 60)
   * **Preview**: Habilita visualizaÃ§Ã£o em tempo real durante processamento
   * **FPS do Preview**: Controla quantos frames/segundo aparecem no preview (5-30)
   * **GPU/CPU**: Escolha o dispositivo de processamento
   * **Tamanho do Modelo**: nano (n), small (s), medium (m), large (l)
   * **DetecÃ§Ã£o de Objetos**: Habilita/desabilita anÃ¡lise de objetos

3. Para configuraÃ§Ãµes avanÃ§adas, clique em **"AvanÃ§ado..."**:
   * Edite limiares de emoÃ§Ãµes (sensibilidade por emoÃ§Ã£o)
   * Ajuste parÃ¢metros de poses (Ã¢ngulos, distÃ¢ncias)
   * Configure pesos contextuais de emoÃ§Ã£o por tipo de cena
   * As alteraÃ§Ãµes sÃ£o salvas em `config/settings.json`

### ConfiguraÃ§Ãµes via Arquivo JSON

Edite diretamente `config/settings.json`:

```json
{
  "frame_skip": 2,
  "target_fps": 30,
  "enable_preview": true,
  "preview_fps": 10,
  "use_gpu": true,
  "model_size": "n",
  "enable_object_detection": true,
  "EMOTION_THRESHOLDS": {
    "neutral": 0.25,
    "sad": 0.60,
    "happy": 0.35,
    "surprise": 0.50,
    "fear": 0.70,
    "angry": 0.50,
    "disgust": 0.55
  }
}
```

### Modo Debug

Ative o checkbox **"Debug"** na toolbar para:

* Ver logs detalhados no console/terminal
* Acompanhar decisÃµes dos detectores em tempo real
* Identificar problemas de detecÃ§Ã£o
* Analisar performance frame a frame

### Requisitos de Hardware (GPU)

Para performance em tempo real, **recomenda-se fortemente o uso de GPU NVIDIA (CUDA)**.

* O sistema detecta automaticamente se `cuda` estÃ¡ disponÃ­vel.
* VocÃª pode forÃ§ar CPU ou GPU nas configuraÃ§Ãµes da interface ou via `--no-gpu` no CLI.

**InstalaÃ§Ã£o PyTorch com CUDA:**

```bash
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121
```

## ğŸ—ï¸ Arquitetura e Fluxo de Processamento

A aplicaÃ§Ã£o segue uma arquitetura modular, onde uma Thread de Processamento (`ProcessorThreadQt`) orquestra a execuÃ§Ã£o sequencial dos modelos de IA frame a frame, sem congelar a interface do usuÃ¡rio.

### Pipeline de Processamento

O fluxo de anÃ¡lise Ã© executado sequencialmente para cada frame processado:

```mermaid
flowchart TD
    Input[ğŸ“¹ VÃ­deo] --> Capture[ğŸ¬ Captura]
    Capture --> Scene[ğŸï¸ Cena]
    Scene --> Pose[ğŸ§ Poses]
    Pose --> OBB[â†ªï¸ OrientaÃ§Ã£o]
    OBB --> Face[ğŸ‘¤ Faces]
    Face --> Emotion[ğŸ˜Š EmoÃ§Ãµes]
    Face --> Object[ğŸ“¦ Objetos]
    Object --> Anomaly[âš ï¸ Anomalias]
    Anomaly --> Gui[ğŸ¨ Interface]
```

| Ordem | MÃ³dulo | FunÃ§Ã£o Principal | Tecnologia |
| :---: | :--- | :--- | :--- |
| **1** | **SceneClassifier** | Identifica o contexto do ambiente (ex: "EscritÃ³rio", "Parque") | YOLO11-cls |
| **2** | **OrientedDetector** | Detecta a orientaÃ§Ã£o de pessoas (em pÃ© vs. deitado) | YOLO11-obb |
| **3** | **ActivityDetector** | Extrai poses esquelÃ©ticas e classifica aÃ§Ãµes | YOLO11-pose |
| **4** | **FaceDetector** | Recorta rostos baseando-se na geometria do corpo | HeurÃ­stica |
| **5** | **EmotionAnalyzer** | Analisa expressÃµes faciais nos recortes | DeepFace |
| **6** | **ObjectDetector** | Detecta objetos e valida coerÃªncia com a cena | YOLO11-detect |
| **7** | **AnomalyDetector** | Aplica regras para identificar comportamentos suspeitos | LÃ³gica |
| **8** | **Visualizer** | Renderiza anotaÃ§Ãµes e atualiza os grÃ¡ficos | OpenCV/Qt |

### Estrutura do Projeto

```text
TC-4/
â”œâ”€â”€ gui_app.py              # Ponto de entrada da aplicaÃ§Ã£o
â”œâ”€â”€ requirements.txt        # Lista de dependÃªncias Python
â”œâ”€â”€ input/                  # DiretÃ³rio para vÃ­deos de entrada
â”œâ”€â”€ output/                 # DiretÃ³rio para vÃ­deos processados
â”œâ”€â”€ reports/                # RelatÃ³rios gerados
â”œâ”€â”€ models/                 # Pesos dos modelos YOLO e DeepFace
â””â”€â”€ src/
    â”œâ”€â”€ config.py           # ConfiguraÃ§Ãµes globais e regras de contexto
    â”œâ”€â”€ gui/                # Interface GrÃ¡fica (PyQt6)
    â”‚   â”œâ”€â”€ main_window_qt.py
    â”‚   â””â”€â”€ threads/processor_thread_qt.py # Orquestrador do pipeline
    â”œâ”€â”€ activity_detector.py # Wrapper YOLO11-pose
    â”œâ”€â”€ emotion_analyzer.py  # Wrapper DeepFace
    â”œâ”€â”€ face_detector.py     # LÃ³gica de extraÃ§Ã£o facial
    â”œâ”€â”€ scene_classifier.py  # Wrapper YOLO11-cls
    â”œâ”€â”€ oriented_detector.py # Wrapper YOLO11-obb
    â”œâ”€â”€ object_detector.py   # Wrapper YOLO11-detect
    â”œâ”€â”€ anomaly_detector.py  # Motor de regras de anomalia
    â””â”€â”€ visualizer.py        # RenderizaÃ§Ã£o visual (OpenCV/PIL)
```

## ğŸ› ï¸ InstalaÃ§Ã£o e ConfiguraÃ§Ã£o

### PrÃ©-requisitos

* **Python 3.10** ou superior (3.12 recomendado).
* **GPU NVIDIA** (Opcional, mas altamente recomendado para performance em tempo real). Drivers CUDA instalados.

### Passo a Passo

1. **Clone o repositÃ³rio e navegue atÃ© a pasta:**

    ```bash
    git clone <url-do-repositorio>
    cd TC-4
    ```

2. **Crie e ative um ambiente virtual:**

    ```bash
    python3 -m venv .venv
    source .venv/bin/activate  # Linux/Mac
    # .venv\Scripts\activate   # Windows
    ```

3. **Instale as dependÃªncias:**

    ```bash
    pip install -r requirements.txt
    ```

    *Nota: A primeira execuÃ§Ã£o baixarÃ¡ automaticamente os modelos YOLO (~100MB cada) e DeepFace.*

4. **Execute a aplicaÃ§Ã£o:**

    ```bash
    python gui_app.py
    ```

## ğŸ“– Como Usar

1. A interface abrirÃ¡ automaticamente.
2. Clique no Ã­cone de **"Abrir Arquivo"** (canto superior esquerdo) para selecionar um vÃ­deo da pasta `input/`.
3. Ajuste as configuraÃ§Ãµes se necessÃ¡rio (botÃ£o "ConfiguraÃ§Ãµes"):
    * **Frame Skip**: Aumente para maior velocidade (ex: 2 ou 3).
    * **Device**: CPU ou CUDA (GPU).
4. Clique no botÃ£o **Play** (â–¶) para iniciar a anÃ¡lise.
5. Acompanhe os resultados em tempo real:
    * **VÃ­deo**: VisualizaÃ§Ã£o com anotaÃ§Ãµes de bounding boxes e labels.
    * **EstatÃ­sticas**: Contadores de faces, anomalias e atividades.
    * **GrÃ¡ficos**: DistribuiÃ§Ã£o de emoÃ§Ãµes e atividades (abas na parte inferior).
6. Ao final, o vÃ­deo processado serÃ¡ salvo na pasta `output/` e um relatÃ³rio de texto em `reports/`.

## âš™ï¸ ConfiguraÃ§Ã£o TÃ©cnica (`src/config.py`)

O arquivo `src/config.py` centraliza constantes importantes, como:

* `SCENE_CONTEXT_RULES`: DicionÃ¡rio que define quais objetos sÃ£o esperados ou anÃ´malos em cada tipo de cena (escritÃ³rio, casa, rua).
* `ANOMALY_THRESHOLDS`: Limiares de sensibilidade para detecÃ§Ã£o de anomalias.
* `EMOTION_THRESHOLDS`: Sensibilidade para cada tipo de emoÃ§Ã£o.

---

## Tech Challenge Fase 4 - PÃ³s Tech Data Analytics
